# 第三章中的ideas

随机梯度下降：抽取一个小批量计算梯度，并更新参数。

均方差损失函数和最大似然估计之间的联系：如果噪声符合正态分布，即误差$\epsilon$符合${1\over \sqrt{2\pi}\sigma}\exp(-{\epsilon^2\over 2\sigma^2})$

取对数之后就比较明显了。

交叉熵的概念：$\sum -p(j)\log q(j)$ $p(j)$是真实分布，而$q(j)$是预测结果概率。我们用这个函数，可以衡量其准确度。

Softmax函数用于将所有概率归一化。Softmax一定是最合理的方式吗？

# 第四章中的ideas

目前常用的激活函数有Sigmoid，tanh，ReLU。但是目前Sigmoid基本不被使用。因为Sigmoid的导数不够大，尤其是远离0时。层数过多时会引起梯度消失。与Sigmoid不同的是，tanh函数是奇函数。导数也存在类似问题。所以现在多用ReLU。

如果网络中没有激活函数层，那么就相当于矩阵相乘而已。堆叠稠密层没有价值。

过拟合的问题，目前主要有两种解决方法。第一种，是引入正则化损失。目的是为了避免某个参数过大，导致某个节点的影响力过高。第二种，是使用暂退法。在反向传播时，故意停止某几个节点的反向传播。这样的目的是为了提高泛化的准确性。因为一部分节点“不存在”，不会影响最终的结果。

正则化损失使用L2范数，在线性回归中就是岭回归；而使用L1范数，则就是套索回归。

前向传播，实际上就是计算的过程。反向传播，则是按照微积分的链式法则进行计算。求梯度的过程是从后向前的，故名之。

梯度爆炸的问题，则是由于网络初始化数值不恰当，导致梯度过大，结果不收敛。

对称性导致神经网络表达性下降的问题：如果某层中（只是举例！）两个神经元计算得到的梯度始终相等，那么这两个神经元的实际作用就相当于一个神经元。我们要用暂退法，正则化来打破这种对称性。

我们可以通过参数初始化的方式，来缓解上述的几个问题。

对于非高难度问题，随机参数初始化即可。或者我们可以使用Xavier初始化。

训练数据对结果的影响：协变量的偏移，标签的偏移和概念的偏移。因此，我们必须要有所动作，想办法减小这种偏移的影响。

# 第六章中的ideas

图像中的不变性：对于物体的识别，应该具有**平移不变性**。同时，在神经网络的前面几层中，应该也具有**局部性**。显然，离得很远的图像之间应当关联较小。

准确地说，卷积运算应该称为互相关。有时也可以喊它特征映射，因为它如同提取出了特征一般。感受野，指的是在前向传播期间可能影响传播的所有元素。

padding和stride，前者为了保持信息量，后者则为了减少冗余。

卷积核通常会设置成奇数，这是为了保证图像两侧处理时的对称性。

通过增加输出通道的方法，可以认为是提取出了不同的特征，进而把这些特征从不同的通道中进行了处理。

1*1卷积层：利用不同的核函数，把不同的特征类型提取到不同的通道中。

池化层的作用：将不同通道的特征提取到同一个神经元中，从而提高图像的识别力。

一种思路是最大池化，用最大的特征值代表全部。另一种思路是使用平均池化，将特征值取平均，从而达到普遍的识别。这样的主要优点是减少卷积层对位置的敏感性。

VGG块： 一堆3*3的卷积层，最后放一个步幅为2的3 *3最大池化层，就是模块化。

NiN块：将全连接层换为两个1*1的卷积层来代替全连接层。在使用时交替使用NiN块以及步幅为2的最大池化层。在最后时使用全局平均池化。

GoogLeNet：含并行连接的网络。Inception块：在同一个块中，有多条通路，从而抽取更多的信息。在不同的通路中，使用了不同尺寸的卷积核。从另一个方面去想，这也是在提取不同大小的信息。

批量规范化：对于一个小输入，我们将中间层的输入输出进行标准化。因为在不同的层之间，它们的数据量级可能有较为显著的区别。表达式为$\gamma\circ {x-\hat\mu\over\sigma}+\beta$。$\gamma,\beta$也是需要学习的参数。为了保证不发生除零错误，我们需要给$\sigma$始终加上一个常量。虽然引入了噪声，但是泛化效果更好。（看上去有点玄学。但现实情况里也确实一直有噪声，也能理解）

批量规范化，在全连接层中的实现，一般放在激活函数之前。卷积层也放在相似的位置上，但是每个层应当拥有自己的拉伸参数和偏移参数。在torch中，我们可以在放置全连接层、卷积层的时候在参数中进行设置(batchnorm)。在预测时，我们用总体的均值和方差来参与计算。

“将现代深度学习的实践比作炼金术” 

楽。现在AI的工作是不是很大程度上在依靠着直觉？

我喜欢依靠直觉，但我不一定有直觉XD

残差层：我们可以把优化模型的过程，看做寻找最优函数的过程。但是，这个最优函数并**不一定**可以被我们的架构取到。所以，我们希望造出一个更好的架构，让它中距离最优函数的“距离”更近些。如果我们能够严格保证我们创新的架构与原先的架构模拟的内容存在严格的包含关系，那么我们就可以认为新的架构确实离目标函数更近些。因此，产生了残差块。残差块的目的是，使得模拟的函数从原先的$f(x)$转变为$f(x)-x$,(因为在输出时还会加上$x$)

稠密块：首先由一串卷积层构成。在每经过一次卷积层后，将输出的结果与输入连接，然后再进入下一层的运算。思想是为了如Taylor展开般，更好地去模拟一个函数。因此，在最后的输出中，通道数会显著增加。因此，要在之后添加一个过渡层，其由1*1卷积层和平均池化层构成，从而降低模型复杂度。

# 第八章中的ideas

我们应当如何处理文本？显然，要把大象放进冰箱，我们要先打开冰箱门，将文本作为字符串加载到内存中。然后，我们将字符串拆分为词元，比如单词和字符。然后，建立一个词表，把词元映射到数字。最后，将文本转换为数字索引序列。

对于语言模型，我们的目的实际上是为了估计$P(x_1,...,x_T)$这一联合概率。那么，对于语言模型的生成问题，我们实际上就是想要找到一个合适的单词，使得$P(x_t|x_{t-1},...,x_1)$有一个较为可观的值。

一种直观的想法，就是统计某一个单词在数据库中出现的总次数，然后分别统计其前一个字母为某一值的总次数。但是，这种方法的精确度很低，因为对于一些不常见的单词组合，不一定找得出合适数量的估计。

一种方法是采用拉普拉斯平滑的方式。它的假设是所有词都会等可能出现。

$$
\hat P(x)={n(x)+\epsilon\over n + m\epsilon}
$$

$n$为总单词数，$m$为出现的不同单词有多少个，$\epsilon$是一个规定的超参数。

如果我们把文本中的词汇做出一些统计后，我们可以看到出现次数最多的词是诸如the、and这类没营养的词。是故，我们想过滤掉这些词，我们把这些词汇称为停用词（stop words）。在过滤掉最前面最高词频的一些词汇后，剩余词汇的出现频率基本满足齐普夫定律。即，第$i$高词频的词出现次数有$n_i\propto {1\over i^\alpha}$

拉普拉斯平滑的颓势于此尽见。从直觉上去想，我们过分高估了尾部单词的出现频率。

除了一元语法词，单词序列基本上也遵循齐普夫定律。

我们如果打算使用神经网络来训练语言模型，那么我们应当对数据进行一定的预处理。我们会把长序列划分为一个个小的子序列。一般有两种采样的方式：随机采样和顺序采样。

我们为什么需要循环神经网络？这是因为如果单词的数量增加，如果我们考虑条件概率，那么我们需要存储的参数数量显然会以指数级别增加。这是不可接受的。因此，我们将过去对现在的影响抽象为一个隐状态。

并且，隐状态之间的转移方程可以抽象为$h_t=f(x_t,h_{t-1})$。我们将每个单词出现的概率近似表示为$P(x_t|x_{t-1},...,x_1)\approx P(x_t|h_{t-1})$

隐状态可以用这个方式计算：$H_t=\phi(XW_1+H_{t-1}W_2+b_1)$

$H_t$表征了从$x_1$到$x_t$之间的所有变量的影响。最终的输出，我们可以记作$O_t=H_tW_3+b_2$

独热编码：将无关变量之间由于编码产生的”连续“分离开来。

梯度裁剪：解决梯度爆炸的问题。即，对于得到的梯度，我们每次反向移动的梯度量为$\min(1,\frac{\theta}{||\vec g||})\vec g$

但是，考虑到循环神经网络中，最后的梯度与每个时刻的值都有关。如果我们对梯度进行完全计算，这在时空上是不可接受的。一般而言，我们只会计算最后几次影响下的梯度，使用这种定长梯度的方法来解决这一问题。还有一种处理方式是随机选取一个长度，选择这些对象影响下的梯度。目前，选取定长的这种做法较为主流。

# 第九章中的ideas

GRU：门控循环单元。是RNN下的一种特殊变体。实际上，我们新增了两种结构，更新门和重置门。在具体实现上，我们在RNN基础上，每次计算出的结构与重置矩阵进行一次元素积，重置矩阵中的取值在0~1之间。值越趋于0，就越会被重置。我们将结果进行偏置等操作后，输出的结果为$Z\circ H_t+(1-Z)\circ H_{t-1}$也就是说，将当前的结果视作两次操作的混合，或者可以把这看做是更新的程度。我们把重置门操作后的情况，称为候选隐状态。将更新门操作后的情况，称为隐状态。

LSTM：长短期循环单元。在这种结构下，我们需要三种门，输入门，遗忘门，输出门。三种门的运算都符合RNN的基本操作，即基于之前的隐状态，进行矩阵乘法以及偏置加法。候选记忆元的计算与前三者无异。其思想与GRU非常接近。区别在于，其将$1-Z$转化为了$I$（输入门）进行运算。之后，在类似于GRU的操作过后，我们使用激活函数进行处理，然后将其与输出门进行元素乘法。因此，我们可以认为LSTM和GRU的区别在于，把三次处理的参数之间的相关性去除，从而提供更高的准确度。这样做可以缓解梯度爆炸和梯度消失的问题。(?)

深度循环神经网络：堆叠多个“并行线”的RNN，RNN中的每个单元不仅依赖于时序上的上一单元，还依赖于层数上的上一个相同位置的单元。

双向循环神经网络：考虑到，完形填空的过程中，我们应该填的词语，不仅和上文有关，也和下文有着很大的关系。所以，产生了双向循环神经网络。

在这个过程中，我们引入了隐马尔可夫模型。每一个时刻的隐藏变量仅依赖于前一个时刻。每一个时刻的输出仅依赖于当前时刻的隐藏变量。在这个过程中，我们可以使用动态规划的办法来辅助计算。在这里，动态规划的主要思想，就是从连加连乘中拆分出一个一个时刻隐变量的求和，和剩余部分之连乘。之后，只要进行递归就可以了。

语言真是一种不好使用的工具......

利用隐马尔可夫模型中的递推，我们就可以建立起双向循环神经网络。

机器翻译：机器翻译是语言**序列转换模型**的关键性问题。解决的是如何将序列从一种语言自动翻译成另外一种语言。在神经网络之前，一般使用的是统计学方法进行。这一部分称为统计机器翻译。也就是说，根据大量的语料库，进行统计，选用最高概率的对象进行翻译。

神经网络机器翻译，则是另外一种不同的方法。

首先是老规矩。我们将数据集按照词元进行划分，从而得到词表。我们定义，如果一个词元出现的次数少于2次，定义为相同的未知词元。这是为了缓解词表大小过大的问题。

同时，为了让语句成为相同的长度，我们会用特定的填充词元，来保证每一个序列的长度是相同的。我们在序列的开始，会统一地添加上一个开始标记，在序列的末尾，统一地添加上一个终止标记。

编码器-解码器架构：编码器的作用，是把任意长度的序列，映射到一个固定长度的序列。解码器的作用，则是将编码器处理后输出的定长序列，重新映射到一个长度可变的序列。编码器-解码器架构，就是把编码器和解码器捆绑在一起，共同组织起一个架构。

序列到序列学习：首先，我们通过编码器，按顺序将输入内容处理并进行编码。在这一过程中，我们引入一个嵌入层，也就是一个矩阵，用来将词元转化为一个特征向量。在翻译的过程中，我们将编码出的内容进行解码，每翻译一个词，就更新目前的隐状态。$h_t=f(x_t,h_{t-1})$然后，我们需要一个函数，来表示编码，即$c=q(h_1,...,h_T)$，此用于表示所谓的上下文变量，一个定长的编码。

之后，我们应当考虑输出的问题。我们可以考察第$t$个输出的对象产生的概率。即：$P(y_t'|y_1,...,y_{t'-1},c)$

为了解决这一点，考虑到RNN的特性，所以我们也跟着定义在输出过程中的隐状态，我们定义为$s_{t'}=g(y_{t'-1},c,s_{t'-1})$，然后运用长短期循环单元中的输出门或者简单的softmax来处理输出。

我们如何来评价一个序列预测的好坏？2002年中提出了BLEU的方法。即：

$$
e^{\min(0,1-{\frac{len_{label}}{len_{pred}}})}\prod_{n=1}^kp_n^{1/2^n}
$$

len表示在标签或预测对象中的词元数。k则是匹配的最长的$n$元词法。也就是，配上的最长长度。$p_i$表示匹配上的$n$元语法与预测所得序列中的$n$元词法数量。为何要用指数的方式进行处理？因为显然，长度越长，与难度之间的关系并不是线性的，难度显著提高。

那么，怎么样进行输出呢？第一种方法就是最朴素的策略，贪心搜索。在每一个时间步处，都选择当前位置上，具有最高**条件概率**出现的词元。但是，这并不一定是一个最优的序列，也就是说，$\prod_{t'=1}^{T'}P(y_{t'}|y_1,...,y_{t'-1},c)$不一定是最大值。如果我们一定要找到最优的序列，那么可以想到的一种方法就是运用穷举进行查找。但这样的复杂度是不可接受的。

一种折中的方式，就是进行束搜索。也就是说，在每一个时间步处，选择k个条件概率乘积最高的词元，并且进一步向下搜索。保证在每个时间步处，都选择k个这样的“最优词元”进行向下的搜索。

# 第十章中的idea

Attention is all you need！

注意力提示：分为自主性的，和非自主性的注意力提示。

自主性提示的含义，可以理解为外界的提示。非自主性提示，指的是key和value之间的映射关系。所有的提示通过attention pooling，从而得到一个好的输出值。非自主性，可以粗略地理解为对一个值的选择倾向。自主性提示区分开了注意力机制和一般的神经网络层。

Nadaraya-Watson核回归：如果我们最终仅用所有输出的平均值进行输出，那么我们显然就忽略了输入的内容与训练集中的输入内容之间的联系。因此，我们可以考虑根据输入的位置对输出进行加权，即：

$$
f(x)=\sum_{i=1}^n{K(x-x_i)\over \sum_{j=1}^nK(x-x_j)}y_i
$$

其中，$K$是核函数。比如说，我们可以选用高斯核函数。也就是说，我们考虑注意力汇聚的情况，对每一个输出的结果根据输入之间的差距进行加权处理。

如果我们的数据量真的很多，那么我们应当可以通过这个非参数化的注意力汇聚模型，得到一个让人满意的输出。

为了进一步提高它的能力，我们可以引入参数，从而更好地调整权值的关系。我们可以令：

$$
f(x)=\sum_{i=1}^n\text{softmax}(-\frac{1}{2}((x-x_i)\omega)^2)y_i
$$

注意力评分函数：

我们可以如此表示注意力汇聚函数$f$:

$$
f(q,(k_1,v_1),...,(k_m,v_m))=\sum_{i=1}^m\alpha(q,k_i)v_i
$$

其中，$\alpha$表示：

$$
\alpha(q,k_i)=\text{softmax}(a(q,k_i))
$$

$a$是注意力评分函数。它可以将查询-键值向量对映射成一个标量。

考虑到，我们在操作词元的过程中，填充了一些无意义的词元。一种可以想到的优化，是指定一个合适的有效序列长度，过滤掉所有超出指定范围位置的词元。

一种注意力评分函数是加性注意力：

$$
a(q,k)=w_v^T\tanh(W_q q+W_kk)
$$

$w_v,W_q,W_k$均为可学习参数。在这里，我们不应当启用偏置。从直觉上看较为合理。

另外一种注意力评分函数是缩放点积注意力：

$$
a(q,k)=q^Tk/\sqrt d
$$

其中，$d$为向量长度。

Bahdanau注意力：之前的机器翻译中，上下文变量$c$的值始终是不变的。如果随着翻译的进行，上下文变量的值发生改变，那么会发生什么事情呢？

RMK:**愚以为，目前机器学习的一个比较常见的优化过程，就是把运算过程中的一些不变量，改为变量，从而使机器进行学习这些参数。比如从GRU到LSTM的过程，把原来统一由更新门进行的工作，分给了多个不同的组件进行工作。是故，此为以时间换能力的一种做法。此依赖于算力的更新迭代。**

Bahdanau注意力模型，采用这样的方式，来更新翻译过程中的上下文变量：

$$
c_{t'}=\sum_{t=1}^T\alpha(s_{t'-1}, h_t)h_t
$$

其中，$\alpha$为加性注意力函数。$s_{t'-1}$是查询。

多头注意力：并行地堆了不同的注意力汇聚模型。让人想到GoogLeNet中的设计。
