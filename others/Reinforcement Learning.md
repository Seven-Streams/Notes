# 第一章 初探强化学习

强化学习，解决的是序贯决策的问题。目标是要找到一个最优的占用度量。占用度量，可以理解为在不同情况下，采取不同策略的概率分布模型。

# 第二章 多臂老虎机(MAB)问题

有一台老虎机，其有k根拉杆，每根拉杆对应不同的奖励概率分布。如何在操作T次拉杆后，获得尽量高的奖励呢？ 

我们引入懊悔(regret)这一概念来刻画拉动一根拉杆的期望奖励和最优拉杆期望之间的差距，定义为二者之差。我们的目标，就是要最小化懊悔。显然，我们需要不断拉一根拉杆，才能对它产生的值有一个大概的估计。我们使用增量式估计来更新，而非重新计算。递推的复杂度更低。

操作实际上可以分为两种类型：探索和利用。

## $\epsilon-$贪婪算法

每次有$1-\epsilon$的概率拉下期望奖励最大的拉杆，否则，就随机选择一根拉杆。$\epsilon$可以设置为随着时间下降。

## 上置信界算法

显然，如果我们对一根拉杆测试较多，那么它的确认度较高，我们更会相信目前的结果是可靠的。原理依靠于霍夫丁不等式，即：

$$
P(E[X]\geq\bar x_n+u)\leq e^{-2nu^2}
$$

我们假设这是第t次操作，拉下的是k号拉杆，那么不确定度为$\hat U(a_t)=\sqrt{\frac{-\log p}{2N(a_t)}}=\sqrt{\frac{-\log p}{2N(k)}}$

根据霍夫丁不等式，$Q(a_t)<\hat Q(a_t)+\hat U(a_t)$成立的概率至少为$1-p$

实际上，在具体操作过程中，我们分母会加2处理，避免出现除0错。同时，可以设置p依旧随时间衰减。由此可见，两种方法的idea还是比较接近的。一般而言。为了更好地控制，我们可以选择$\max[\hat Q(a)+c\cdot \hat U(a)]$的那根拉杆。

## 汤普森采样算法

我们采用贝塔分布来给每一个选择的概率进行建模。我们根据目前已经建模的情况，进行一次随机采样。然后，我们选择采样结果最高的那一个。

# 第三章 马尔可夫决策过程

## 马尔可夫过程

随机过程(sticgastuc process)是我们研究的对象。如果一个状态转移的过程中，下一状态仅取决于当前状态，那么就称这个过程具有马尔可夫性质。

我们称具有马尔可夫性质的随机过程为马尔可夫过程，用$<\mathcal{S},\mathcal{P}>$来描述马尔可夫过程。其中，$\mathcal{S}$是状态集，$\mathcal{P}$是状态转移矩阵，有$\mathcal{P}_{ij}=P(s_j|s_i)$

根据状态转移矩阵，我们可以轻松地画出状态之间的转移关系。

## 马尔可夫奖励过程

在马尔可夫过程的基础上，加上奖励函数$r$和折扣因子$\gamma$,即得到马尔可夫奖励过程，记作$<\mathcal{S},\mathcal{P},r,\gamma>$

$r(s)$表示转移为该状态时可以获得的奖励的期望。$\gamma$是为了控制短期影响和长期影响之间的权重关系。

假设我们考察从$t$时刻到最后的最后，定义$R_t$为第$t$时刻的可以获得的奖励，那么我们定义回报为：

$$
G_t=\sum_{k=0}^\infin \gamma^k R_{t+k}
$$

我们定义，一个状态的期望回报称为这个状态的价值。

由于这是一个马尔可夫过程，所以我们有：

$$
V(s)=E[G_t|S_t=s]\\
=E[R_t+\gamma G_{t+1}|S_t=s]\\
=E[R_t+\gamma V(S_{t+1})|S_t=s]
$$

所以，我们有：

$$
V(s)=r(s)+\gamma\sum_{s'\in \mathcal{S}}P(s'|s)V(s')
$$

这就是贝尔曼方程。如果我们将每个状态的奖励和回报都写作列向量，那么，我们可以得到：

$$
V=R+\gamma PV\\
V=(I-\gamma P)^{-1}R
$$

这个方法很好。但是它的复杂度是$O(|S|^3)$，这是不可接受的。

## 马尔可夫决策过程

在马尔可夫奖励过程的基础上，加上一个外来的“智能体”(agent)，可以对当前的状态做出影响，就得到了马尔可夫决策过程，记作$<\mathcal{S},\mathcal{A},P,r,\gamma>$，其中$\mathcal{A}$为所有动作的集合。此后，我们采用状态转移函数，而非状态转移矩阵来描述这一过程。因为自变量增多了。

我们定义智能体的策略为$\pi(a|s)=P(A_t=a|S_t=s)$

类似地扩充，我们紧接着定义$V^\pi(s)=E_\pi[G_t|S_t=s]$

因为我们的选择还与动作有关，所以我们额外定义一个动作价值函数$Q^\pi(s,a)=E_\pi[G_t|S_t=s,A_t=a]$

二者之间存在着这样的联系：$V^\pi(s)=\sum_{a\in\mathcal{A}}\pi(a|s)Q^\pi(s,a)$

$Q^\pi(s,a)=r(s,a)+\gamma\sum_{s'\in\mathcal{S}}P(s'|s,a)V^\pi(s')$

这两个式子是符合直觉的。

对于二者分别取期望，可以得到贝尔曼期望方程：

$$
Q^\pi(s,a)=r(s,a)+\gamma\sum_{s'\in\mathcal{S}}P(s'|s,a)\sum_{a'\in\mathcal{A}}\pi(a'|s')Q^\pi(s',a')\\
V^\pi(s)=\sum_{a\in\mathcal{A}}\pi(a|s)(r(s,a)+\gamma\sum_{s'\in\mathcal{S}}P(s'|s,a)V^\pi(s')
$$

仔细看看这两个式子，还是可以理解其朴素的意图的。

### 蒙特卡罗方法

通过模拟爆算，近似地试探出每个状态的价值函数。我们采样很多条序列。在一个序列计算结束后，我们根据每个状态经过时之总回报进行平均处理，得到一个对状态价值的估计。在大量采样之后，我们在对每条序列中某一个状态的价值进行平均取样。根据大数定律，这样会趋近于$V^\pi(s)$。

## 占用度量

因为不同的策略，会使得agent访问到不同的概率分布的状态，所以我们需要考察在某一策略下的状态访问分布。我们定义$P_t^\pi(s)$表示策略$\pi$使得$t$时刻状态为$s$的概率。下式中的$1-\gamma$用于归一化概率。初始状态分布记为$v_0(s)$

$$
v^\pi(s)=(1-\gamma)\sum_{t=0}^\infin\gamma^t P_t^\pi(s)\\
v^\pi(s')=(1-\gamma)v_0(s')+\gamma\int P(s'|s,a)\pi(a|s)v^\pi(s)dsda
$$

这里需要稍体察其用意。

定义占用度量为

$$
\rho^\pi(s,a)=(1-\gamma)\sum_{t=0}^\infin\gamma^t P_t^\pi(s)\pi(a|s)
$$

这可以用来表征一个状态-动作对被访问到的概率。二者之间存在这样的关系

$$
\rho^\pi(s,a)=v^\pi(s)\pi(a|s)
$$

Theorem1：二者的占用度量相等，当且仅当策略相同。

Theorem2：给定一可行的占用度量，可生成其的唯一策略是$\pi_\rho={\rho(s,a)\over \sum_{a'}\rho(s,a')}$

也就是说，这可以认为是个不动点？

## 最优策略

这是强化学习的目标。一个策略是最优策略，当且仅当在任意状态下，它都不比其他策略差，那么它就是最优策略。我们可以定义一系列最优函数：

$$
V^*(s)=\max_\pi V^\pi(s)\\
Q^*(s,a)=\max_\pi Q^\pi(s,a)\\
Q^*(s,a)=r(s,a)+\gamma\sum_{s'\in\mathcal{S}}P(s'|s,a)V^*(s')\\
V^*(s)=\max_{a\in\mathcal{A}}Q^*(s,a)
$$

第三式的产生，是由最优性保证的。

根据二者之间的关系，我们可以得到贝尔曼最优方程：

$$
V^*(s)=\max_{a\in\mathcal{A}}\{r(s,a)+\gamma\sum_{s'\in\mathcal{S}}P(s'|s,a)V^*(s')\}\\
Q^*(s,a)=r(s,a)+\gamma\sum_{s'\in\mathcal{S}}P(s'|s,a)\max_{a'\in\mathcal{A}}Q^*(s',a')
$$

# 第四章 动态规划算法

主要有两种基于动态规划的强化学习算法：策略迭代(policy iteration)和价值迭代(value iteration)。策略迭代由策略评估(policy evaluation)和策略提升(policy improvement)两部分共同构成。

## 悬崖漫步环境

策略评估：用上一轮得到的结果来更新每一个策略的状态价值函数。即

$$
V^{k+1}(s)=\sum_{a\in\mathcal{A}}\pi(a|s)(r(s,a)+\gamma\sum_{s'\in\mathcal{S}}P(s'|s,a)V^k(s'))
$$

测试轮数足够多的话，会有$V^k(s)\approx V^\pi(s)$

策略提升：在测试的过程中，对策略本身进行改进。想法是如果进行了足够次数的测试后，如果有：

$$
Q^\pi(s,\pi'(s))\geq V^\pi(s)
$$

对任意$s$恒成立，那么

$$
V^{\pi '}(s)\geq V^\pi
$$

此为策略提升定理。这一点从直觉上去考虑是成立的。

策略迭代的过程，就是测试、策略评估、策略提升依次进行的过程。

价值迭代也是类似的过程。即

$$
V^{k+1}(s)=\max_{a\in\mathcal{A}}\{r(s,a)+\gamma\sum_{s'\in\mathcal{S}}P(s'|s,a)V^k(s')\}
$$

# 第五章 时序差分算法

$$
V(s_t)\leftarrow V(s_t)+\alpha[G_t-V(s_t)]
$$

这是蒙特卡洛算法中对状态价值函数的更新。由于我们需要$G_t$,所以必须要计算完整个序列才能进行改动。为了在更新过程中，就可以对其进行改动，我们可以这样处理：

$$
V(s_t)\leftarrow V(s_t)+\alpha[r_t+\gamma V(s_{t+1})-V(s_t)]
$$

我们称$r_t+\gamma V(s_{t+1})-V(s_t)$为**时序差分误差**。

Sarsa算法：类似于状态价值的更新，我们可以用这样的方式进行迭代：

$$
Q(s_t,a_t)\leftarrow Q(s_t,a_t)+\alpha[r_t+\gamma Q(s_{t+1},a_{t+1})-Q(s_t,a_t)]
$$

在每一个状态下，我们可以贪心地选择动作价值最大的那一种选择进行计算。这样的计算过程中，是实时进行更新的。但是，如果我们始终选择贪心算法所指明的策略，很可能会有不少状态-动作对没有得到探索。因此，我们可以参考在MAB问题中的思想，引入一个概率值$\epsilon$，有一定的概率选择其他动作，在剩下的概率中选择目前最好的动作并进行更新。这就是Sarsa算法的思想。

这样的方案下，每一次计算所得的结果，相较于蒙特卡洛方法而言，方差应该是小不少的。但是，这样的方式是有偏的，可能会导致最终结果的出入。为了弥补这一点，我们引入了多步Sarsa算法，即迭代过程中，考察多步的价值。即：

$$
Q(s_t,a_t)\leftarrow Q(s_t,a_t)+\alpha[r_t+\sum_{i=1}^n\gamma^iQ(s_{t+i},a_{t+i})-Q(s_t,a_t)]
$$

Q-learning算法与Sarsa算法非常相似。它的更新迭代方式仅有一些微小的不同，即：

$$
Q(s_t,a_t)\leftarrow Q(s_t,a_t)+\alpha[r_t+\gamma\max_{a}Q(s_{t+1},a)-Q(s_t,a_t)]
$$

在决策过程中，依旧采用类似Sarsa算法中的$\epsilon$贪心算法进行决策的选取。随后再在所有决策中，选择最大值。

所以，Sarsa算法和Q-learning算法之间的显著区别，就是前者在计算价值时，必须采用当次探索时的行为结果，但是Q-learning算法则并不是这样。我们把Sarsa这样的算法称为在线策略算法，而把Q-learning这样的算法称为离线策略算法。

我们称，采样数据的策略为行为策略，称用采样数据来更新的策略为目标策略。在线策略，表示行为策略即目标策略，而离线策略，表示行为策略和目标策略不是同一个策略。

# 第六章 Dyna-Q算法

强化学习算法分为两种：基于模型的强化学习和无模型的强化学习。

基于模型的话，就要先对环境建模。比如状态转移概率，奖励模型等。而无模型的强化学习则不用考虑这些。无模型的强化学习直接学习概率，而不用学习这个模型。无模型强化学习采样效率低，但是简单，性能较好；基于模型的强化学习，采样效率较高。第四章中的方法，就是基于模型的；第五章中的算法，就是不基于模型的。

Dyna-Q算法采用一种被称为Q-planning的方法来基于模型生成数据。

基本思路是这样的：进行$\epsilon$采样，更新$Q(s,a)$,并且基于当且获得的奖励去更新原有模型$M(s,a)$。然后，进行一定数量的循环，随机选择一个曾经访问过的状态，并且采取一个曾经执行过的策略，后用$M(s_m,a_m)$去更新$r_m$和$s'_m$。话说的比较抽象，但实际上是好理解的：就是探索，更新当前节点数据，在用这个节点数据去更新一部分已经探测过的数据。

# 第七章 DQN算法

考察到动作价值矩阵$Q$，会随着数据规模的增大，呈多项式速度增长。是故，之前的动作价值矩阵$Q$应当不是我们最终运用的方法。最终，我们要用函数去拟合动作价值矩阵$Q$。

DQN算法，解决的是在连续状态下，动作有限的问题。

在DQN算法中，我们引入车杆环境来模拟这一点。什么意思呢？就是一辆小车上有一根杆子。agent的目标，是要想办法尽量保持杆处于竖直的状态。如果杆倾角过大，或者车的移动距离过大，我们就认为agent失败了。

考察到，在Q-learning算法中，我们为了要对函数进行更新，我们需要取出该状态下，采取某一动作可以获得的奖励最大值。是故，动作必须是离散的，换言之，动作的数量必须是有限个。

一切的一切，还是归结到了神经网络。我们可以使用神经网络来拟合$Q$函数。我们假设，神经网络的参数集合为$\omega$,我们用$Q_\omega(s,a)$来表示在这个神经网络下，状态动作对的价值。

我们定义损失函数如下：

$$
\omega^*=\text{arg}\min_\omega\frac{1}{2N}\sum_{i=1}^N[Q_\omega(s_i,a_i)-(r_i+\gamma\max_{a'\in\mathcal{A}}Q_{\omega}(s_{i+1},a'))^2]
$$

我们称之为深度Q网络。从直觉上看看，这个损失函数貌似在刻画自洽性？

经验回放的概念：将每次采样得到的四元组数据存储在回放缓冲区中。在之后的训练中，也会从回放缓冲区中随机地挑选一些样本来参与数据更新。这样做有两个目的。一方面，可以提高样本的复用率。另一方面，考虑到我们在一次采用过程中的样本不是独立的，适当地加入一些回放的样本，可以提高独立性，使之可以满足独立性假设。

在实际的实现过程中，我们一般会使用两个网络。一个，是实时更新的网络$\omega$，我们称之为训练网络；一个，是每隔$C$步才会和训练网络参数进行一次同步的网络。我们称之为目标网络$\omega_-$。在计算损失函数的过程中，我们使用目标网络的参数，也就是上式中的第二项改变为$\omega_-$；在更新的过程中，我们更新训练网络的参数。

# 第八章 DQN改进算法

## Double DQN算法

一般的DQN算法，会出现过高估计的问题。为了解决这一问题，有Double DQN算法。

在上面的更新中，我们用了

$$
(r_i+\gamma\max_{a'\in\mathcal{A}}Q_{\omega}(s_{i+1},a'))
$$

来更新数据。在Double DQN算法中，我们改为：

$$
(r_i+\gamma Q_{\omega_-}(s',\text{arg}\max_{a'\in\mathcal{A}}Q_{\omega}(s_{i+1},a'))
$$

也就是说，我们用一套网络来选策略，用一套网络来估值，从而来减少过高估计的问题。（玄学？）

## Dueling DQN算法

我们定义优势函数为：

$$
A(s,a)=Q(s,a)-V(s)
$$

在同一个状态下，所有动作的优势值之和为0(？？？写错了吧，怎么想都不会是对的吧。说的应当是按照策略概率加权后的结果。或者说，优势的期望值是0)。

建模为：

$$
Q_{\eta,\alpha,\beta}(s,a)=V_{\eta,\alpha}(s)+A_{\eta,\beta}(s,a)
$$

其中$\eta$是状态价值函数和优势函数共享的网络参数，$\alpha,\beta$为状态价值函数和优势函数的参数。从而，我们将$Q$值分解为了对两个函数参数的计算，分别获得价值函数和优势函数。这样做的目的是，有些时候，我们可能并不关注不同动作之间价值的差异。

为了保证建模的唯一性，我们定义，优势函数最高的一个对象的优势值为0。

# 第九章 策略提升算法

在之前的操作中，我们尝试去构造一个价值函数，从而对每个状态进行比较合理化的估值。在这里，我们可以考虑对策略本身进行建模。

我们定义，$\pi_\theta$为策略函数参数为$\theta$下的策略。

我们的目标，应当是最大化$J(\theta)=E_{s_0}[V^{\pi_\theta}(s_0)]$

其中$s_0$是初始状态。

通过推导，我们可以得到：

$$
\Delta_\theta J(\theta)\propto E_{\pi_\theta}[Q^{\pi_\theta}(s,a)\nabla_\theta\log\pi_\theta(a|s)]
$$

这个推导过程还是比较复杂的。

那么，我们可以从初始样本开始采样，随后根据梯度对参数进行调整。

因为，有：

$$
Q^{\pi_\theta}=\sum_{t=0}^T(\sum_{t'=t}^T\gamma^{t'-t}r_{t'})
$$

所以我们可以将这一部分代换，从而不必对状态价值函数进行建模。

# 第十章 Actor-Critic算法

在第九章的策略提升算法中，我们采用随机采样的方式（蒙特卡洛方式？）来进行策略的更新。在Actor-Critic算法中，会尝试将采样的结果拟合成一个值函数去指导策略的学习。

在上面，我们提到过：

$$
\Delta_\theta J(\theta)\propto E_{\pi_\theta}[Q^{\pi_\theta}(s,a)\nabla_\theta\log\pi_\theta(a|s)]\\
Q^{\pi_\theta}=\sum_{t=0}^T(\sum_{t'=t}^T\gamma^{t'-t}r_{t'})
$$

如果将上面式子中的$Q^{\pi_\theta}$改写为另外的函数，那么我们就得到了另外一种类型的“梯度”。可以将其替换为时序差分残差$r_t+\gamma V^{\pi_\theta}(s_{t+1})-V^{\pi_\theta}(s_t)$,或者是重新估计一个函数。(rmk:莫名又联想到了DL中的一些想法。认为一部分参数“不准确”，从而通过计算的方式来更精确地刻画，最终达到提升结果的目的。)

或者说，在这个过程中，我们实际上让网络尝试去记忆那些曾经取样过的数据。我们要训练两个网络，一个就是策略的网络，另外一个是拟合价值函数的神经网络。这里依旧使用了梯度下降的做法。

# 第十一章 TRPO算法

针对之前提到的策略提升算法，在参数更新的过程中，可能因为参数调整幅度过大，导致整体性能下降。所以，TRPO算法的思路是先确认一个置信域，在当前参数的某一个高维球邻域内，则信任之。

经过推导，有：

$$
J(\theta')-J(\theta)=\frac{1}{1-\gamma}E_{s\sim v^{\pi_{\theta'}}}E_{a\sim\pi_{\theta'}(\cdot|s)}
[A^{\pi_{\theta}}(s,a)]
$$

就我简单的理解而言，第一个期望，表征的是预期在经过一段时间后，我们在哪个状态；第二个期望，则是表征在这个状态下的预期动作。其中，$\theta$为原参数，而$\theta'$为新参数。但是这个方程求解起来并不简单，因为等式的右侧用到了我们要用于求解的$\theta'$。

因此，我们进行了一步近似操作。如果我们的策略并没有发生显著的改变，那么我们可以近似认为第一个期望在两组参数下，结果近似相同。所以，我们可以将上式转化为：

$$
\frac{1}{1-\gamma}E_{s\sim v^{\pi_{\theta}}}E_{a\sim\pi_{\theta'}(\cdot|s)}
[A^{\pi_{\theta}}(s,a)]
$$

rmk：对于这个转换，感觉只能用常识上去解释？或者从参数$\to$策略$\to$动作$\to$状态这个链条上去看，改变参数，对动作的影响会更大些，对于状态的影响可能会偏小？(给我一种先射箭再画靶的感觉XD)

只需要简单地进行下变换，就可以得到：

$$
\frac{1}{1-\gamma}E_{s\sim v^{\pi_{\theta}}}E_{a\sim\pi_{\theta}(\cdot|s)}
[\frac{\pi_{\theta'}(a|s)}{\pi_\theta(a|s)}A^{\pi_{\theta}}(s,a)]
$$

我们用上面的式子来优化参数。并且，要求两组策略的KL散度小于一定的数值。
