# Lec 2

## 熵

记号约定：$X$是离散随机分布。$\mathcal{X}$为字母表。

我们用如下的方法定义熵：

$$
H(X)=-\sum_{x\in X}p(x)\log p(x)
$$

显然当$p(x)=0$时，不会提供任何信息。

一般此时$\log$的底数都取2。底数为2，称为bits。底数为e，称为nats。

$$
0\leq H(X)\leq\log|\mathcal{X}|
$$

第一个等号成立时，为不可能事件或者必然事件。

第二个等号成立时，为均一分布。

概率论的链式法则和贝叶斯规则：

$$
p(x_1,...,x_n)=p(x_n)p(x_{n-1}|x_n)...p(x_1|x_2,...,x_n)\\
p(x)p(y|x)=p(y)p(x|y)
$$

## 联合熵

对于多个随机变量的分布。

定义为：

$$
H(X,Y)=-\sum_{x\in\mathcal{X}}\sum_{y\in\mathcal{Y}}p(x,y)\log p(x,y)
$$

显然有：

$H(X,X)=H(X)$

$H(X,Y)=H(Y,X)$

## 条件熵

$X=x$已知，$p(Y=y|X=x)$也是一个概率密度函数。

$$
H(Y|X)=\sum_{x\in\mathcal{X}}p(x)H(Y|X=x)\\
=-\sum_{x\in\mathcal{X}}p(x)\sum_{y\in\mathcal{Y}}p(y|x)\log p(y|x)\\
=-\sum_{x\in\mathcal{X}}p(x,y)\sum_{y\in\mathcal{Y}}\log p(y|x)\\
=-E\log p(Y|X)
$$

从直觉的角度而言，条件熵已经有一定的限定了，所以有

$$
H(Y|X)\leq H(Y)\\
H(X|Y)\not=H(Y|X)\\
H(X|Y)+H(Y)=H(Y|X)+H(X)=H(X,Y)
$$

第三个等式是符合直觉的。

## 零熵

如果一个随机变量$X$相对另一个随机变量$Y$的条件熵为0，则$X$是$Y$的一个函数。这是符合直觉的。说明$X$的取值一直和$Y$有关。

# Lec 3

## 相对熵（K-L距离）

$$
D(p||q)=\sum_{x\in \mathcal{X}}p(x)\log {p(x)\over q(x)}\\
=E_p\log {p(X)\over q(X)}
$$

相对熵永远是非负的。

$$
D(p||q)=E_p\log p(X)-E_p\log q(X)
$$

容易观察到，相对熵并不是一种度量。首先，相对熵显然就不具有对称性。

$$
D(p||q)=0\iff p=q
$$

Pinsker不等式 

$$
D(p||q)\geq{1\over 2\ln 2}V^2(p,q)
$$

## 互信息

$$
I(X;Y)=\sum_x\sum_yp(x,y)\log {p(x,y)\over p(x)p(y)}\\
=D(p(x,y)||p(x)p(y))\\
=E_{p(x,y)}\log{p(X,Y)\over p(X)p(Y)}
$$

 互信息符合对称性。$I(X,X)=H(X)$。这两部分都是符合直觉的。

可以用这样非正式的集合方法表示信息之间的关系：

$$
H(X,Y)=H(X)\cup H(Y)\\
I(X;Y)=H(X)\cap H(Y)\\
H(X|Y)=H(X)/I(X;Y)\\
H(Y|X)=H(Y)/I(X;Y)
$$

这样的相对关系，是非常符合直觉的。

$$
\log{p(X,Y)\over p(X)p(Y)}=-\log p(X)-\log p(Y)+\log p(X,Y)
$$

从而可以证明

$$
I(X;Y)=H(X)+H(Y)-H(X,Y)
$$

互信息可以在一定程度上刻画出变量之间的相关性。

## 熵的链式法则

$$
H(X_1,X_2,...,X_n)=H(X_1)+H(X_2|X_1)+...+H(X_n|X_{n-1},...,X_1)
$$

## 条件互信息

$$
I(X;Y|Z)=H(X,Z)-H(X|Y,Z)
$$

$$
I(X_1,...,X_n;Y)=\sum_{i=1}^nI(X_i;Y)
$$

## 条件相对熵

$$
D(p(y|x)||q(y|x))=E_{p(x,y)}\log {p(Y|X)\over q(Y|X)}
$$

$$
D(p(x,y)||q(x,y))=D(p(x)||q(x))+D(p(y|x)||q(y|x))
$$

# Lec 4

## 熵的独立界

$$
H(X_1,...,X_n)\leq \sum_{i=1}^nH(X_i)
$$

这一点是符合直觉的。等号成立当且仅当均为独立变量。

## 马尔科夫链

假设$X\to Y\to Z$构成一个马尔科夫链，则

$$
p(x,y,z)=p(x)p(y|x)p(z|y)
$$

马尔科夫过程应当是具有可逆性的。

$$
I(X;Z|Y)=0\\
I(X;Y|Z)=E_{p(X,Y,Z)}\log{p(X,Y|Z)\over p(X|Z)p(Y|Z)}
$$

### 数据处理不等式

$$
I(X;Y)\geq I(Y;Z)
$$

在如上马尔科夫链的假设上。

此外，我们可以定义：

$$
I(X;Y;Z)=I(X;Y)-I(X;Y|Z)
$$

## Fano不等式

我们打算去估计随机变量$X$的分布$p(x)$

我们观察到了与$X$相关的变量$Y$,概率分布为$p(y|x)$

$$
P_e=Pr(\hat X=X)\\
H(P_e)+P_e\log|\mathcal{X}|\geq H(X|\hat X)\geq H(X|Y)
$$

 我们定义误差的随机变量：

$$
E=0,\hat X=X;E=1,\hat X\not=X
$$

故，当$\hat X=X$之时，$P_e=0$,故在计算过程中可以略去这一项。

是故，我们有：

$$
H(P_e)+P_e\log(|\mathcal{X}-1|)\geq H(X|\hat X)
$$

# Lec 5

给出一系列随机变量$X_1,X_2,...$，我们称这个系列收敛于随机变量$X$，当以下三种条件其一符合时：

$$
\exist n,\forall \epsilon>0,Pr\{|X_n-X|>\epsilon\}\to0\\
\exist n,E(X_n-X)^2\to0\\
Pr\{\lim_{n\to\infin}X_n=X\}=1
$$

对于上述记号，当$A$是一个集合时，定义为：

$$
Pr\{X=A\}=\sum_{a\in A}Pr\{X=a\}
$$

大数定律：n个独立的随机变量$X_1,...X_n$符合$p(x) $

强大数定律：$Pr\{\lim_{n\to\infin}\bar X_n=E(X)\}=1$

弱大数定律：$\bar X_n\to E(X)$

## AEP（渐进均分性）

令$X_1,...,X_n$为$i.i.d$的随机变量，其分布均符合$p(x)$，则：

$$
-{1\over n}\log(p(X_1,...,X_n))=-{1\over n}\sum_{i=1}^n\log p(X_i)\to H(X)
$$

注意到上式的底数为2。取指数后，并取出随机变量的事件，我们就可以得到一些符合下面这个关系的事件：

$$
2^{-n(H(X)-\epsilon)}\leq p(x_1,...,x_n)\leq2^{-n(H(X)+\epsilon)}
$$

我们把这样的事件组成一个集合，称作典型集，记作$A_\epsilon^{(n)}$。典型集中的事件总数约为$2^{nH}$

典型集中事件发生的概率之和趋于1。

我们定义高概率集为最小的，满足$Pr(X=B_\delta^{(n)})\geq 1-\delta$的集合。

## 数据压缩

对数据首先进行编码，之后再进行解码。

定义$P_e=P(X^n\neq \hat X^n)$

# Lec 6

## 随机过程

如果n个随机变量之间存在相互联系呢？那么我们之后将引入熵率。

在随机过程中，各变量的观测是有顺序的。

eg：赌徒的破产，马尔科夫过程。即：

$$
X_{i+1}=X_i\underline+1
$$

### 稳态过程

$$
Pr\{X_1=x_1,...,X_n=x_n\}=Pr\{X_{1+l}=x_1,...,X_{n+l}=x_n\}
$$

也就是说，系统在时间上具有不变性。

故

$$
p(X_1)=p(X_2)=...=p(X_n)\\
p(X_1,X_3)=p(X_2,X_4)
$$

## 马尔科夫过程

$$
P(X_{n+1}=x_{n+1}|X_n=x_n,...,X_1=x_1)=Pr(X_{n+1}=x_{n+1}|X_n=x_n)
$$

马尔科夫链具有时间不变性。

当$p(X_{n+1})=p(X_n)$时，马尔科夫过程是一个稳态。

$$
p(x_{n+1})=\sum_{x_n}p(x_n)P_{x_nx_{n+1}}=x^TP=x^T
$$

## 熵率

对于一个有时序的随机过程，熵率可以这样定义：

$$
H(\mathcal{X})=\lim_{n\to\infin}{1\over n}H(X_1,...,X_n)
$$

由链式法则，

$$
H(X_n,...,X_1)=\sum_{i=1}^nH(X_i|X_{i-1},...,X_1)
$$

我们可以令右侧这个累加式中，每一项为$a_n$,随后验证其收敛性。

注意到，右侧这一条件熵每一项是递减的。并且，熵始终大于0。

定义：$H'(\mathcal{X})=\lim_{n\to\infin}H(X_n|X_{n-1},...,X_1)$

因此，熵率在数值上等于$H'(\mathcal{X})$

考虑马尔科夫过程的熵率。由于马尔科夫链具有无后效性，所有其熵率就是$H(X_2|X_1)$

## 热力学第二定律

我们可以把孤立系统看做是一个马尔科夫过程。

## 马尔科夫过程的函数

$X_1,...,X_n$是一个马尔科夫过程。

$Y_1,...,Y_n,Y_i=\phi(X_2)$

首先，我们可以找到条件熵的下界，也就是$H(\mathcal{Y})$

然后，我们可以找到$H(\mathcal{Y})$一个下界，也就是$H(Y_n|Y_{n-1},...,Y_1,X_1)$

两边同取极限即可。

# Lec 7

定义：编码的非奇异性。也就是编码和事件之间存在一个双射。

定义：前缀码，每一个编码都不是其他编码的前缀。

对一组前缀码，存在Kraft不等式：

$$
\sum_{i=1}^mD^{-i}\leq 1
$$

$D$是字母表的大小。$i$是每一个编码的长度。如果等号不成立，那么编码是有冗余的；如果等号成立，那么我们称其为完备码；若该等式不成立，那么相应的编码并不是唯一可解编码。

我们为什么需要前缀码？这是因为当传输信息时，我们不再需要按照固定长度（比如8个bit）来解码，而是可以直接从前往后，根据编码树解码。根据前缀码的性质，这样的解释是唯一的。Kraft不等式就在描述这样一件事情。

关于最优码，我们总是希望编码的平均长度越短越好。

## 香农编码

假设一个事件出现的概率为$p(i)$，我们令其对应事件的编码码长为$\lceil-\log p(i)\rceil$。从小到大进行编码。它也是前缀码，但不是最优码。只有当概率分布满足一定条件时，方可有紧致性。

# Lec 8

## Huffman编码

每次取出D个出现概率最低的编码，在当前位合并为一个编码，从而使期望编码最少。我们依次这样操作下去。直到当前位置只有两个节点为止。

为了Huffman编码最长编码情况出现的比较小，我们应当加入一些假符号，使其概率值为0，从而使总符号数为$1+k(D-1)$

我们称一个分布是D-adic的，如果每一个事件出现的概率均为$D^{-n},n\in N$

如果一个码字，没有兄弟姐妹，也就是说，其为父节点之唯一子节点，那么显然，我们可以删除最后一位。

## Shannon-Fano-Elias编码

事件无需按照发生概率的大小进行分布。我们定义一个累积概率函数，为该符号此前出现的所有发生事件的概率之和。然后，我们再在此基础上加上当前事件发生概率之一半。将此函数值表达为$D$进制之小数，把小数点后的后$\lceil p(x)\rceil+1$位作为其编码。

用数值来表示事件的idea。

# Lec 9

## 随机变量的生成

用二叉树来作为算法的表示树。每一个节点要么为叶子节点，要么就有两个儿子节点。

所有的事件都位于叶子节点处。在每一个节点处，向左儿子移动和向右儿子的概率是相等的。当每一个事件与叶子节点之间形成双射之时，树的期望深度与该随机变量的信息熵相等。如若不然，则树的期望深度不会小于信息熵的大小。

如果一个事件发生的概率并非$1\over 2$的正整数次幂，那么我们可以将其发生的概率改写为二进制，随后分解进行表示。

## 通用信源编码

在实际生活中，我们常常会不知道不同符号的概率分布，那么我们就没有办法按照概率来编码。

是故，在这个过程中，我们想要完成的事情，就是最小化我们估计的某一事件概率和该事件实际概率之间的差距。也就是最小化最大冗余。

冗余可以转化为信道的容量进行计算。

## 算术编码

算术编码依旧不能脱出需要已知概率的桎梏。它承袭了Shannon-Fanos-Elias编码的思想。首先，其将$[0,1)$按照概率之分布，划分为对应之区间。倘若某一数字在某符号对应之区间中，则说明这一位便是它了。此后，再将每一区间按概率层层划分，依此法进行划分，从而解码得到结果。

## LZ算法

此为一种无损压缩之方式。它使用了一个滑动窗口进行编码，在这个滑动窗口中，包含两部分内容。第一部分是已编码部分，称为查找缓冲区。第二部分是未编码部分，称为先行缓冲区。然后，在查找缓冲区中检索，是否有先行缓冲区中的第一个字符。然后，我们用一个三元组来表示之。三元组中的内容，包含偏移量（从查找缓冲区的哪里复制？）、长度（需要复制多少个字符？）、后继（下一个字符是什么？）。倘若没有搜索到，那么记录为$<0,0,C(...)>$，…代表的就是下一个字符。

显然，需要滑动窗口的大小是相同的，这样才可以保证其正确性。

# Lec 10

我们可以用状态转移矩阵来模拟在传输过程中数据遇到的噪声。

## 离散无记忆信道（DMC）

我们希望发送出的信息$W$与接收方解码后得到的信息$\hat W$尽可能相近。

DMC，之前通信过的内容不会影响状态转移矩阵。

我们定义信道容量为$C=\max_{p(x)}I(X;Y)$

$X$为发送的信息，$Y$为接收方收到的信息。

$C\geq0,C\leq \log\mathcal{X},C\leq\log\mathcal{Y}$

一般情况下，写不出信道容量的显式表达式。

# Lec 11

信息集：可能发送的信息的集合。

对于每一个信息，我们可以用它们的序号来表示之。

无记忆性和反馈：

信道的第n次扩展：已经传输过n次信息后的情况。假设收到的信息为$y_k$

，发出的信息为$x_k$,我们称信道为无记忆性的，当：

$p(y_k|x_k,y_{k-1})=p(y_k|x_k)$

我们称一个信道是无反馈的，当：

$p(x_k|x_{k-1},y_{k-1})=p(x_k|x_{k-1})$

从直观上去想，就是发送的内容与对方接收到的内容无关。

如果无记忆且无反馈，那么

$p(y_n|x_n)=\Pi_{i=1}^np(y_i|x_i)$

$H(Y_n|X_n)=\sum_{i=1}^nH(Y_i|X_i)$

## DMC

信息集与序号之间建立起了一个双射，从而得到了一个码本。

我们假设信息集中有$M$个元素。那么，我们定义码率为$\log M\over n$,$n$为码长。

定义：条件错误概率：

$\lambda_i=Pr(g(Y^n)\neq i|X^n =x^n(i))=\sum_{y^n}p(y^n|x^n(i))I(g(y^n)\neq i)$

其中，$I$是示性函数。其表示发送信息i时，发生错误的概率。

最大错误概率，就是条件错误概率之最大值。

平均错误概率，则为取平均。

我们称一个码率是可取的，指存在一个序列，使得在这种编码条件下，最大错误概率趋于零。

联合典型性与联合AEP

信道编码定理：对于一个信道容量$C$，待传的信息率为$R$，只要$R<C$，则这个码率一定是可取的。

# Lec 12

如果传输过程中没有错误，假设发送前信息为$W$,编码后为$X^n$,传输后为$Y^n$,解码后为$\hat W$

$$
nR=H(W)=H(W|Y^n)+I(W;Y^n)=I(W;Y^n)
$$

这是由于传输过程中无反馈性。

故

$$
I(W;Y^n)\leq I(X^n;Y^n)\leq nc
$$

如果有错误的话：

$$
nR=H(W)=H(W|\hat W)+I(W;\hat W)\leq1+P^{n}_\epsilon nR+I(W;\hat W)\\
\leq 1+P^{n}_\epsilon nR+I(X^n;Y^n)
$$

通过联合典型性解码，$\hat W$应当满足：

$(X^n(\hat W),Y^n)$具有联合典型性。

没有其他的$W',s.t.(X^n(W'),Y^n)\in A_\epsilon^{n}$

这样就可以唯一解码了。

在DMC中，假设有反馈性，反馈不能增大信道容量。

信源信道分离：

在信息压缩过程中，有$R>H$

在信息传输过程中，有$R<C$

故，一定有$H<C$

信源信道编码定理：发送随机变量$v$,$H(V)<C$时，方存在以种信源信道编码，使得总误差趋向于0。

纠错码：重复码、奇偶校验码，汉明码

# Lec 13

微分熵

我们设一个变量$x$的cdf函数(cumulative distribution function)为$F(x)=Pr(X\leq x)$

如果$F(x)$是连续的，那么我们成这个变量是连续随机变量。

我们记$f(x)=F'(x)$为随机变量的概率密度曲线。

我们定义连续性随机变量的微分熵$h(X)=-\int_S f(x)\log f(x)dx$

其中，$S$为随机变量的支持集，即$f(x)>0$的集合。

高斯分布的微分熵为${1\over 2}\log (2\pi e\sigma^2)$

一个连续性随机变量携带了无穷的信息(考虑0~1的均匀分布，将对象写成二进制，则每一位的信息熵为1，总体信息熵为无穷)。微分熵并不能用来衡量信息的多少。

$h(aX)=h(X)+\log|a|\\ h(AX)=h(X)+\log|\det A|$

为了将连续型的随机变量与离散型的随机变量挂钩，我们可以规定落在某一个区间内的对象，都取同一个函数值。这个函数值通过积分中值定理，保证该区间内积分值不变，从而维护总概率为1。我们记这时的离散随机变量为$X^\Delta$

$p(X^\Delta=x_i)=\int_{i\Delta}^{(i+1)\Delta}f(x)dx=f(x_i)\Delta$

所以

$H(X^\Delta)=-\sum\Delta f(x_i)\log f(x_i)-\log\Delta$

我们也可以定义连续型随机变量的AEP和典型集。

联合微分熵$-\int f(x^n)\log f(x^n)dx^n$

条件微分熵$-\int f(x,y)\log f(x|y)dxdy$

协方差矩阵

我们定义两个随机变量的协方差为$cov(X;Y)=E(X-EX)E(Y-EY)=E(XY)-(EX)(EY)$

对于一组随机变量组成的向量$X=[x_1,...,X_n]^T$我们定义其协方差矩阵为$E(X-EX)(X-EX)^T$

我们定义关联矩阵为$EXX^T$，$EXX^T=E(X-EX)(X-EX)^T+(EX)(EX^T)$

协方差矩阵是对称的。

对于多元正态分布$N$，有

$f(x)={1\over (\sqrt{2\pi})^n|K|^{1\over 2}}e^{-{1\over2}(x-\mu)^TK^{-1}(x-\mu)}$

这是非常复杂的一坨公式...$K$是协方差矩阵。

其微分熵和一元高斯分布时非常相近。为${1\over 2}\log (2\pi e)^n|K|$

对于两个连续型随机变量，我们定义其相对熵$D(f||g)$为$\int f\log{f\over g}$

对于互信息，$I(X;Y)=\int f(x,y)\log{f(x,y)\over f(x)f(y)}dxdy$

互信息依旧是非负的。微分熵可以有负值。

# Lec 14

 最大熵原理：

$h(X)\leq{1\over2}\log(2\pi e\sigma^2)$

当$X$符合正态分布时，熵最大。

平衡信息不等式：

$h(X,Y)\leq h(X)+h(Y)\\ h(X,Y,Z)\leq {1\over2}h(X,Y)+{1\over2}h(Y,Z)+{1\over2}h(Z,X)$

等。动机是为了保证式子两边的变量总权重是一样的。这里可以是离散的，可以是连续的。

Han's不等式：

考察指标集$\{1,...,n\}$中之一集合$S$。我们记$X(S)$为下标在$S$中的$x_i$之集合。

记$h^{(n)}_k={1\over C^k_n}\sum_{|S|=k}{h(X(S))\over k}$

$g_k^{(n)}={1\over C^k_n}\sum_{|S|=k}{h(X(S)|X(S^C))\over k}$

显然有$h_k^{(n)}=g_k^{(n)}$

通过平衡信息不等式，$h_k^{(n)}$随k增大而增大。

由联合熵和条件熵的性质,$g_k^{(n)}$随k增大而减少。

Fisher信息：$I(X)=\int_{-\infin}^{+\infin}f(x)[{{\partial\over\partial x}f(x)\over f(x)}]^2dx$

也是针对连续变量提出的。

# Lec 15

连续信道。最具代表性的一种：高斯信道。

如果是时间离散信道，我们认为输出是输入和噪声的和（废话）。噪声符合标准正态分布。

最主要的限制一般是能量限制。

对于码字的长度，我们一般限制为：

$$
{1\over n}\sum_{i=1}^n x_i^2\leq P
$$

高斯信道的信道容量为$C=\max_{f(x)\leq EX^2\leq P}\leq I(X;Y)$

记噪声的方差为$N$，则

$C={1\over 2}\log(1+{P\over N})$

我们可以认为，每一个码字附近可以看做一个超球体。因为噪声符合标准正态分布，所以有很高的概率，接收端的信息在发出内容的球体附近。

我们希望，我们的小球之间彼此并不相交。

高斯信道的定义：我们认为所有事件在一个事件集内。事件和编码存在一一映射。满足上述的性质，存在解码函数。如果接收方接收的信息落在了唯一球体中，那么OK。否则，则认为出错。

平行高斯信道：多个高斯信道，并且它们的噪声是不同的。我们希望知道，在这种情况下的信道容量是多大。

$C=\max_{f(x_1,x_2,...,x_k)\leq E\sum x_i^2\leq P}I(X_1,...,X_k;Y_1...,Y_k)$

# Pre by Stargazer

考虑一个二元序列，两个符号出现的概率分别为5%、95%。在这种情况下进行Huffman编码，就是没有压缩。

考虑算术编码。

Byte Pair Encoding

把一些出现频率高的视作同一个词元，从而进行压缩。
