# 第三章中的ideas

随机梯度下降：抽取一个小批量计算梯度，并更新参数。

均方差损失函数和最大似然估计之间的联系：如果噪声符合正态分布，即误差$\epsilon$符合${1\over \sqrt{2\pi}\sigma}\exp(-{\epsilon^2\over 2\sigma^2})$

取对数之后就比较明显了。

交叉熵的概念：$\sum -p(j)\log q(j)$ $p(j)$是真实分布，而$q(j)$是预测结果概率。我们用这个函数，可以衡量其准确度。

Softmax函数用于将所有概率归一化。Softmax一定是最合理的方式吗？

# 第四章中的ideas

目前常用的激活函数有Sigmoid，tanh，ReLU。但是目前Sigmoid基本不被使用。因为Sigmoid的导数不够大，尤其是远离0时。层数过多时会引起梯度消失。与Sigmoid不同的是，tanh函数是奇函数。导数也存在类似问题。所以现在多用ReLU。

如果网络中没有激活函数层，那么就相当于矩阵相乘而已。堆叠稠密层没有价值。

过拟合的问题，目前主要有两种解决方法。第一种，是引入正则化损失。目的是为了避免某个参数过大，导致某个节点的影响力过高。第二种，是使用暂退法。在反向传播时，故意停止某几个节点的反向传播。这样的目的是为了提高泛化的准确性。因为一部分节点“不存在”，不会影响最终的结果。

正则化损失使用L2范数，在线性回归中就是岭回归；而使用L1范数，则就是套索回归。

前向传播，实际上就是计算的过程。反向传播，则是按照微积分的链式法则进行计算。求梯度的过程是从后向前的，故名之。

梯度爆炸的问题，则是由于网络初始化数值不恰当，导致梯度过大，结果不收敛。

对称性导致神经网络表达性下降的问题：如果某层中（只是举例！）两个神经元计算得到的梯度始终相等，那么这两个神经元的实际作用就相当于一个神经元。我们要用暂退法，正则化来打破这种对称性。

我们可以通过参数初始化的方式，来缓解上述的几个问题。

对于非高难度问题，随机参数初始化即可。或者我们可以使用Xavier初始化。

训练数据对结果的影响：协变量的偏移，标签的偏移和概念的偏移。因此，我们必须要有所动作，想办法减小这种偏移的影响。

# 第六章中的ideas

图像中的不变性：对于物体的识别，应该具有**平移不变性**。同时，在神经网络的前面几层中，应该也具有**局部性**。显然，离得很远的图像之间应当关联较小。

准确地说，卷积运算应该称为互相关。有时也可以喊它特征映射，因为它如同提取出了特征一般。感受野，指的是在前向传播期间可能影响传播的所有元素。

padding和stride，前者为了保持信息量，后者则为了减少冗余。

卷积核通常会设置成奇数，这是为了保证图像两侧处理时的对称性。

通过增加输出通道的方法，可以认为是提取出了不同的特征，进而把这些特征从不同的通道中进行了处理。

1*1卷积层：利用不同的核函数，把不同的特征类型提取到不同的通道中。

池化层的作用：将不同通道的特征提取到同一个神经元中，从而提高图像的识别力。

一种思路是最大池化，用最大的特征值代表全部。另一种思路是使用平均池化，将特征值取平均，从而达到普遍的识别。这样的主要优点是减少卷积层对位置的敏感性。

VGG块： 一堆3*3的卷积层，最后放一个步幅为2的3 *3最大池化层，就是模块化。

NiN块：将全连接层换为两个1*1的卷积层来代替全连接层。在使用时交替使用NiN块以及步幅为2的最大池化层。在最后时使用全局平均池化。

GoogLeNet：含并行连接的网络。Inception块：在同一个块中，有多条通路，从而抽取更多的信息。在不同的通路中，使用了不同尺寸的卷积核。从另一个方面去想，这也是在提取不同大小的信息。

批量规范化：对于一个小输入，我们将中间层的输入输出进行标准化。因为在不同的层之间，它们的数据量级可能有较为显著的区别。表达式为$\gamma\circ {x-\hat\mu\over\sigma}+\beta$。$\gamma,\beta$也是需要学习的参数。为了保证不发生除零错误，我们需要给$\sigma$始终加上一个常量。虽然引入了噪声，但是泛化效果更好。（看上去有点玄学。但现实情况里也确实一直有噪声，也能理解）

批量规范化，在全连接层中的实现，一般放在激活函数之前。卷积层也放在相似的位置上，但是每个层应当拥有自己的拉伸参数和偏移参数。在torch中，我们可以在放置全连接层、卷积层的时候在参数中进行设置(batchnorm)。在预测时，我们用总体的均值和方差来参与计算。

“将现代深度学习的实践比作炼金术” 

楽。现在AI的工作是不是很大程度上在依靠着直觉？

我喜欢依靠直觉，但我不一定有直觉XD

残差层：我们可以把优化模型的过程，看做寻找最优函数的过程。但是，这个最优函数并**不一定**可以被我们的架构取到。所以，我们希望造出一个更好的架构，让它中距离最优函数的“距离”更近些。如果我们能够严格保证我们创新的架构与原先的架构模拟的内容存在严格的包含关系，那么我们就可以认为新的架构确实离目标函数更近些。因此，产生了残差块。残差块的目的是，使得模拟的函数从原先的$f(x)$转变为$f(x)-x$,(因为在输出时还会加上$x$)

稠密块：首先由一串卷积层构成。在每经过一次卷积层后，将输出的结果与输入连接，然后再进入下一层的运算。思想是为了如Taylor展开般，更好地去模拟一个函数。因此，在最后的输出中，通道数会显著增加。因此，要在之后添加一个过渡层，其由1*1卷积层和平均池化层构成，从而降低模型复杂度。

# 第八章中的ideas

我们应当如何处理文本？显然，要把大象放进冰箱，我们要先打开冰箱门，将文本作为字符串加载到内存中。然后，我们将字符串拆分为词元，比如单词和字符。然后，建立一个词表，把词元映射到数字。最后，将文本转换为数字索引序列。

对于语言模型，我们的目的实际上是为了估计$P(x_1,...,x_T)$这一联合概率。那么，对于语言模型的生成问题，我们实际上就是想要找到一个合适的单词，使得$P(x_t|x_{t-1},...,x_1)$有一个较为可观的值。

一种直观的想法，就是统计某一个单词在数据库中出现的总次数，然后分别统计其前一个字母为某一值的总次数。但是，这种方法的精确度很低，因为对于一些不常见的单词组合，不一定找得出合适数量的估计。

一种方法是采用拉普拉斯平滑的方式。它的假设是所有词都会等可能出现。

$$
\hat P(x)={n(x)+\epsilon\over n + m\epsilon}
$$

$n$为总单词数，$m$为出现的不同单词有多少个，$\epsilon$是一个规定的超参数。

如果我们把文本中的词汇做出一些统计后，我们可以看到出现次数最多的词是诸如the、and这类没营养的词。是故，我们想过滤掉这些词，我们把这些词汇称为停用词（stop words）。在过滤掉最前面最高词频的一些词汇后，剩余词汇的出现频率基本满足齐普夫定律。即，第$i$高词频的词出现次数有$n_i\propto {1\over i^\alpha}$

拉普拉斯平滑的颓势于此尽见。从直觉上去想，我们过分高估了尾部单词的出现频率。

除了一元语法词，单词序列基本上也遵循齐普夫定律。

我们如果打算使用神经网络来训练语言模型，那么我们应当对数据进行一定的预处理。我们会把长序列划分为一个个小的子序列。一般有两种采样的方式：随机采样和顺序采样。

我们为什么需要循环神经网络？这是因为如果单词的数量增加，如果我们考虑条件概率，那么我们需要存储的参数数量显然会以指数级别增加。这是不可接受的。因此，我们将过去对现在的影响抽象为一个隐状态。

并且，隐状态之间的转移方程可以抽象为$h_t=f(x_t,h_{t-1})$。我们将每个单词出现的概率近似表示为$P(x_t|x_{t-1},...,x_1)\approx P(x_t|h_{t-1})$

隐状态可以用这个方式计算：$H_t=\phi(XW_1+H_{t-1}W_2+b_1)$

$H_t$表征了从$x_1$到$x_t$之间的所有变量的影响。最终的输出，我们可以记作$O_t=H_tW_3+b_2$

独热编码：将无关变量之间由于编码产生的”连续“分离开来。

梯度裁剪：解决梯度爆炸的问题。即，对于得到的梯度，我们每次反向移动的梯度量为$\min(1,\frac{\theta}{||\vec g||})\vec g$

但是，考虑到循环神经网络中，最后的梯度与每个时刻的值都有关。如果我们对梯度进行完全计算，这在时空上是不可接受的。一般而言，我们只会计算最后几次影响下的梯度，使用这种定长梯度的方法来解决这一问题。还有一种处理方式是随机选取一个长度，选择这些对象影响下的梯度。目前，选取定长的这种做法较为主流。

# 第九章中的ideas

GRU：门控循环单元。是RNN下的一种特殊变体。实际上，我们新增了两种结构，更新门和重置门。在具体实现上，我们在RNN基础上，每次计算出的结构与重置矩阵进行一次元素积，重置矩阵中的取值在0~1之间。值越趋于0，就越会被重置。我们将结果进行偏置等操作后，输出的结果为$Z\circ H_t+(1-Z)\circ H_{t-1}$也就是说，将当前的结果视作两次操作的混合，或者可以把这看做是更新的程度。我们把重置门操作后的情况，称为候选隐状态。将更新门操作后的情况，称为隐状态。

LSTM：长短期循环单元。在这种结构下，我们需要三种门，输入门，遗忘门，输出门。三种门的运算都符合RNN的基本操作，即基于之前的隐状态，进行矩阵乘法以及偏置加法。候选记忆元的计算与前三者无异。其思想与GRU非常接近。区别在于，其将$1-Z$转化为了$I$（输入门）进行运算。之后，在类似于GRU的操作过后，我们使用激活函数进行处理，然后将其与输出门进行元素乘法。因此，我们可以认为LSTM和GRU的区别在于，把三次处理的参数之间的相关性去除，从而提供更高的准确度。这样做可以缓解梯度爆炸和梯度消失的问题。(?)

深度循环神经网络：堆叠多个“并行线”的RNN，RNN中的每个单元不仅依赖于时序上的上一单元，还依赖于层数上的上一个相同位置的单元。

双向循环神经网络：考虑到，完形填空的过程中，我们应该填的词语，不仅和上文有关，也和下文有着很大的关系。所以，产生了双向循环神经网络。

在这个过程中，我们引入了隐马尔可夫模型。每一个时刻的隐藏变量仅依赖于前一个时刻。每一个时刻的输出仅依赖于当前时刻的隐藏变量。在这个过程中，我们可以使用动态规划的办法来辅助计算。在这里，动态规划的主要思想，就是从连加连乘中拆分出一个一个时刻隐变量的求和，和剩余部分之连乘。之后，只要进行递归就可以了。

语言真是一种不好使用的工具......

利用隐马尔可夫模型中的递推，我们就可以建立起双向循环神经网络。

机器翻译：机器翻译是语言**序列转换模型**的关键性问题。解决的是如何将序列从一种语言自动翻译成另外一种语言。在神经网络之前，一般使用的是统计学方法进行。这一部分称为统计机器翻译。也就是说，根据大量的语料库，进行统计，选用最高概率的对象进行翻译。

神经网络机器翻译，则是另外一种不同的方法。

首先是老规矩。我们将数据集按照词元进行划分，从而得到词表。我们定义，如果一个词元出现的次数少于2次，定义为相同的未知词元。这是为了缓解词表大小过大的问题。

同时，为了让语句成为相同的长度，我们会用特定的填充词元，来保证每一个序列的长度是相同的。我们在序列的开始，会统一地添加上一个开始标记，在序列的末尾，统一地添加上一个终止标记。

编码器-解码器架构：编码器的作用，是把任意长度的序列，映射到一个固定长度的序列。解码器的作用，则是将编码器处理后输出的定长序列，重新映射到一个长度可变的序列。编码器-解码器架构，就是把编码器和解码器捆绑在一起，共同组织起一个架构。

序列到序列学习：首先，我们通过编码器，按顺序将输入内容处理并进行编码。在这一过程中，我们引入一个嵌入层，也就是一个矩阵，用来将词元转化为一个特征向量。在翻译的过程中，我们将编码出的内容进行解码，每翻译一个词，就更新目前的隐状态。$h_t=f(x_t,h_{t-1})$然后，我们需要一个函数，来表示编码，即$c=q(h_1,...,h_T)$，此用于表示所谓的上下文变量，一个定长的编码。

之后，我们应当考虑输出的问题。我们可以考察第$t$个输出的对象产生的概率。即：$P(y_t'|y_1,...,y_{t'-1},c)$

为了解决这一点，考虑到RNN的特性，所以我们也跟着定义在输出过程中的隐状态，我们定义为$s_{t'}=g(y_{t'-1},c,s_{t'-1})$，然后运用长短期循环单元中的输出门或者简单的softmax来处理输出。

我们如何来评价一个序列预测的好坏？2002年中提出了BLEU的方法。即：

$$
e^{\min(0,1-{\frac{len_{label}}{len_{pred}}})}\prod_{n=1}^kp_n^{1/2^n}
$$

len表示在标签或预测对象中的词元数。k则是匹配的最长的$n$元词法。也就是，配上的最长长度。$p_i$表示匹配上的$n$元语法与预测所得序列中的$n$元词法数量。为何要用指数的方式进行处理？因为显然，长度越长，与难度之间的关系并不是线性的，难度显著提高。

那么，怎么样进行输出呢？第一种方法就是最朴素的策略，贪心搜索。在每一个时间步处，都选择当前位置上，具有最高**条件概率**出现的词元。但是，这并不一定是一个最优的序列，也就是说，$\prod_{t'=1}^{T'}P(y_{t'}|y_1,...,y_{t'-1},c)$不一定是最大值。如果我们一定要找到最优的序列，那么可以想到的一种方法就是运用穷举进行查找。但这样的复杂度是不可接受的。

一种折中的方式，就是进行束搜索。也就是说，在每一个时间步处，选择k个条件概率乘积最高的词元，并且进一步向下搜索。保证在每个时间步处，都选择k个这样的“最优词元”进行向下的搜索。

# 第十章中的idea

Attention is all you need！

注意力提示：分为自主性的，和非自主性的注意力提示。

自主性提示的含义，可以理解为外界的提示。非自主性提示，指的是key和value之间的映射关系。所有的提示通过attention pooling，从而得到一个好的输出值。非自主性，可以粗略地理解为对一个值的选择倾向。自主性提示区分开了注意力机制和一般的神经网络层。

Nadaraya-Watson核回归：如果我们最终仅用所有输出的平均值进行输出，那么我们显然就忽略了输入的内容与训练集中的输入内容之间的联系。因此，我们可以考虑根据输入的位置对输出进行加权，即：

$$
f(x)=\sum_{i=1}^n{K(x-x_i)\over \sum_{j=1}^nK(x-x_j)}y_i
$$

其中，$K$是核函数。比如说，我们可以选用高斯核函数。也就是说，我们考虑注意力汇聚的情况，对每一个输出的结果根据输入之间的差距进行加权处理。

如果我们的数据量真的很多，那么我们应当可以通过这个非参数化的注意力汇聚模型，得到一个让人满意的输出。

为了进一步提高它的能力，我们可以引入参数，从而更好地调整权值的关系。我们可以令：

$$
f(x)=\sum_{i=1}^n\text{softmax}(-\frac{1}{2}((x-x_i)\omega)^2)y_i
$$

注意力评分函数：

我们可以如此表示注意力汇聚函数$f$:

$$
f(q,(k_1,v_1),...,(k_m,v_m))=\sum_{i=1}^m\alpha(q,k_i)v_i
$$

其中，$\alpha$表示：

$$
\alpha(q,k_i)=\text{softmax}(a(q,k_i))
$$

$a$是注意力评分函数。它可以将查询-键值向量对映射成一个标量。

考虑到，我们在操作词元的过程中，填充了一些无意义的词元。一种可以想到的优化，是指定一个合适的有效序列长度，过滤掉所有超出指定范围位置的词元。

一种注意力评分函数是加性注意力：

$$
a(q,k)=w_v^T\tanh(W_q q+W_kk)
$$

$w_v,W_q,W_k$均为可学习参数。在这里，我们不应当启用偏置。从直觉上看较为合理。

另外一种注意力评分函数是缩放点积注意力：

$$
a(q,k_i)=q^Tk_i/\sqrt d
$$

其中，$d$为向量长度。

Bahdanau注意力：之前的机器翻译中，上下文变量$c$的值始终是不变的。如果随着翻译的进行，上下文变量的值发生改变，那么会发生什么事情呢？

RMK:**愚以为，目前机器学习的一个比较常见的优化过程，就是把运算过程中的一些不变量，改为变量，从而使机器进行学习这些参数。比如从GRU到LSTM的过程，把原来统一由更新门进行的工作，分给了多个不同的组件进行工作。是故，此为以时间换能力的一种做法。此依赖于算力的更新迭代。**

Bahdanau注意力模型，采用这样的方式，来更新翻译过程中的上下文变量：

$$
c_{t'}=\sum_{t=1}^T\alpha(s_{t'-1}, h_t)h_t
$$

其中，$\alpha$为加性注意力函数。$s_{t'-1}$是查询。

多头注意力：并行地堆了不同的注意力汇聚模型。让人想到GoogLeNet中的设计。

自注意力和位置编码：如果查询、键、值来自于同一组输入，那么我们称这种注意力为自注意力。自注意力机制和CNN相比，感受野更大，从而易于获取更多信息。但是，它需要大数据（因为不像CNN那样捕捉到了平移不变性等），并且在长序列训练时复杂度较高。但是，它容易捕捉到长程的信息，并且最长路径短。和RNN相比。它具有并行的优势。自注意力输出为：

$$
y_i=f(x_i,(x_1,x_1),...,(x_n,x_n))
$$

$x_i$是query,$x_i$同时也作为key和value。

考虑到，在机器翻译过程中，使用自注意力方式，不像RNN那样，可以保持原有的位置信息。所以，在编码时，我们给每个词元添加**位置编码**。假定$X\in \R^{n\times d}$，表示有$n$个词元，$d$维的嵌入表示。那么，位置编码的矩阵大小亦为此。并且，有：

$$
p_{i,2j}=\sin(\frac{i}{10000^{2j/d}})\\
p_{i,2j+1}=\cos(\frac{i}{10000^{2j/d}})
$$

我们称之为$P$。将之加于$X$上。

这两个式子在做什么？我们观察下，可以发现，同一列的元素，位于相同的一个正弦波段上。并且，同一列上，从上到下，大小之间有着明显的区别。这样做的合理性在哪里？比如，我们可以观察二进制下0到7：

$$
000\\
001\\
010\\
011\\
100\\
101\\
110\\
111
$$

可以看到，位数越高，0-1之间振荡的频率越小。因此，这种位置编码的一个想法，就是要模拟这种趋势。采用浮点数存储，比二进制方法节省空间。并且，这样的位置编码还可以捕获相对位置信息。这是一个线性代数的计算，在这里就不记录了。![](C:\Users\Lincher\Desktop\notes\other%20lectures\Transformer.jpg)

Transformer模型。直接上图吧。图中的“加”表示这里是残差神经网络的类似方法，即加上原有的自变量。规范化，指的是将输出矩阵除以一特定值，避免梯度爆炸。逐位前馈网络，指的是全连接层、ReLU层和全连接层的组合。掩蔽多头注意力和普通的多头注意力有什么区别？一个主要的想法是，在输出的序列中，前面的内容不应当改变对后面内容的注意力。(rmk:对吗？)

# 第十二章 计算机视觉

## 图像增广

所谓图像增广，指的是对数据图片进行一定变换，从而降低一些属性（比如位置）对结果的影响。

这一操作是为了提高模型的泛化性。

它的思路就是为了减少模型对不该关注的对象的无意义关注。比如说，把一只猫旋转90度，它还是一只猫。或者，我们将一只猫的图片，截取一部分，作为人类，我们依旧可以认出来这是猫的一部分。光线的明暗，猫的毛色，都不应该成为我们无法辨认出这只动物是猫的理由。

## 微调

运用场景：我们要分辨椅子的种类。但是椅子的种类比较少，尽管每种椅子的照片样本是很多的。

微调，就是为了解决在这种情况下模型分类能力较差的问题。首先，我们把原来的模型照抄过来，把参数原封不动地拷贝一遍，除了输出层。在输出层上，我们进行随机的初始化。这样做的一个直觉就是，在更大的数据集上，模型可能学习到了一些关于图形的纹理特征等通用信息，但是，只不过在输出层上可能要进行较大幅度的重新组合。

所以，我们仅使用一个全新的输出层，此前的对象保持不变。

由于输出层是全新的，所以，我们需要提高输出层的学习率。

## 目标检测和边界框

我们使用矩形边界框来“框”住对象。

## 锚框

如果需要人手工去标注边界框的数据，那未免过分愚蠢了些。因此，我们需要目标检测算法，来帮助我们进行查询，查找这些区域中，是否包含着我们的目标，并且对边界框进行进一步的调整。

所谓锚框，指的是以每个像素为中心，生成多个缩放比和宽高比不同的边界框，称之为锚框。

假定输入图片的高度为h，定义生成图片宽高比为r，缩放比为s,则锚框宽度为$hs\sqrt r$，高度为$hs/\sqrt r$

生成不同的锚框，可以有不同的缩放比。为了降低取样数量，我们一般会挑选一个缩放比，在这个缩放比下多次改变宽高比采样；再挑选一个宽高比，在这个宽高比下多次改变缩放比进行采样。

我们使用交并比的概念，来判断锚框的覆盖效果。

我们定义杰卡德系数为：$\frac{|A\cap B|}{|A\cup B|}$，即交并比。

以下是一种把真实边界框分配给锚框的算法。

假设我们有$n_a$个锚框，有$n_b$个边界框。定义一个$n_a*n_b$的矩阵，第i行第j列的元素表示$A_i$与$B_j$的交并比。

首先，我们找到矩阵中最大的元素$x_{ij}$，把真实边界框$B_j$分配给$A_i$。然后，我们丢弃掉第$i$行和第$j$列中的所有元素。

我们重复这一操作，直到矩阵已经为空。对于多于的锚框，我们记进行遍历。如果这些锚框和某些真实边界框的交并比达到一个阈值，我们就把这个边界框分配给这个锚框。

hmm。感觉很平凡的一种方法。

在这样分配了真实边界框后，我们就可以给每一个锚框进行类别上的标记了。我们标记锚框的类别，和被分配到的真实边界框类别相同。我们从中心坐标、高度、宽度这几个方面来标记锚框和真实边界框之间的偏移量。比较常见的有这种方法：

$$
(\frac{\frac{x_b-x_a}{\omega_a}-\mu_x}{\sigma_x},
\frac{\frac{y_b-y_a}{\omega_a}-\mu_y}{\sigma_y},
\frac{\log\frac{\omega_b}{\omega_a}-\mu_\omega}{\sigma_\omega},
\frac{\log\frac{h_b}{h_a}-\mu_h}{\sigma_h}
)
$$

$\mu$和$\sigma$均为一些特定的常数。

如果一个锚框没有被分配仍和的真实边界框，那么我们将锚框的类别标记为背景。我们将所有的背景类别的锚框称为负类锚框。剩余的，我们称其为正类锚框。我们不关心负类锚框的偏移量。

如果锚框的数量过多，可能会输出许多较为相似的预测边界框，其围绕着相同的目标。为了解决这一重复计算的问题，我们可以采用非极大值抑制的方法（当初只分配一个最像的会好吗？还是因为样本不够，预测不够准确？）

首先，我们选取置信度最高的预测边界框，也就是该边界框类别的确定程度。我们将所有非背景类的边界框按置信度降序排列，生成列表。从L的开头开始选取，遍历一遍，取出和该预测边界框交并比过大的所有边界框。重复这一过程，直到结束。

## 多尺度目标检测

对于一张图片而言，就算按照锚框的取样策略，以整张图片上的每一个像素为中心，想要按照锚框采样策略将锚框全部采样一遍，数量上还是太多了。所以，我们首先可以采用均匀分布的方式，均匀地选择一些中心点。并且，考虑到小的物体对象更难以捕获，我们可以多采样一些小的锚框，少采样一些大的锚框。

可以通过这样的方式进行实现。我们称卷积层输出的二维数组为特征图。

假定特征图的尺寸为$h\times w$，那么我们就均匀地在原图上选取$h\times w$个点，进行锚框的生成操作。假设我们总共有$c$张这样的特征图。

## 单发多框检测(SSD)

SSD算法以一个基础网络块，和多个多尺度特征块叠加而成。每一个块都会根据多尺度目标检测的方式进行类别的预测和边界框的检测。

类别预测层：假设总共有$q$类物体，那么我们应当分出$q+1$个类别。在类别预测层中，使用了一个保持了输入的高和宽的卷积层，即padding为1的3*3卷积层。假设在每个位置上，我们会生成$a$个锚框。这样，可以保证输出的结果和该层中的锚框存在一一对应关系。因为有$q+1$个类别，所以我们应当最终产生$a(q+1)$个通道。并且，通道上的固定位置，即表示了对应采样点的结果。

边界框预测层：与类别预测层相同，但是输出结果应当表征锚框的偏移量，所以输出通道数是$4a$

高宽减半块：由两个padding为1的3*3卷积层和步幅为2的2 *2最大池化层构成。

基础网络块：为了提取特征。在示例中，采用了三个高宽减半块，并且每通过一次高宽减半块，就翻倍输出的通道数量。

## R-CNN

首先，在不同尺度下选取多个提议区域，可以具有不同形状和大小。然后，通过一个预训练之卷积网络，提取特征。用其他模型对提取出的特征分类。用线性回归模型来预测区域内特征对应的边界框偏移量。

Fast R-CNN：调换了卷积网络和选取提议区域之间的顺序。先对整张图片进行特征提取，然后再选取提议区域。然后，使用RoI将提议区域中的特征转化为形状相同的区域。从而便于在连接后输出。然后，通过一个全连接层变换形状，之后进行类别预测和边界框预测。

Faster R-CNN：将提议区域的选取方式从选择性选取转换为区域提议网络。区域提议网络由一个卷积层、选取锚框、类别分类和边界框偏移量预测、非极大值抑制等过程构成。

Mask R-CNN：基于Faster R-CNN，但是把RoI换为了兴趣区域对齐层。这里运用了双线性插值法来保留空间信息。兴趣区域对齐层会额外连接一个全卷积层，从而进行掩码预测。

双线性插值法：单线性插值是平凡的。如果我们已知空间中一个矩形的四个顶点处函数值，那么我们可以在$x$方向分别进行一次单线性插值。将所得到的插值点在$y$方向上进行一次线性插值，这就是双线性插值。

选取提议区域的一种方法：选择性搜索。首先，把图片分为很多小块。然后，计算相邻两个块之间的相似程度。如果足够相似，则合并之，最终得到可能具有某种特征的物体。

## 转置卷积

转置卷积操作上有点类似于对卷积进行反向操作，然其并非卷积的逆操作。具体地说，就是在原特征图上的每个位置，和卷积核进行乘法运算，然后将其映射到后一张输出的特征图上的，大小与转置卷积核相同的对应范围内。这样做，可以起到类似重建特征图尺寸的效果。
