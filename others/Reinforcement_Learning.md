# Reinforcement Learning

## Lec 1

强化学习是一种在交互过程中**试错**迭代的技术。

- 预测型任务

- 决策型任务

动态环境：环境有可转移的状态，并且有多步决策。

强化学习一般针对于动态黑盒的环境。

序贯决策（动态环境）

### 定义

在交互中学习来实现目标

- 感知：感知环境的状态

- 动作：采取行为来影响状态或者达到目标

- 目标：奖励函数最大化

在每一步，智能体获得观察 $O_t$，执行动作$A_t$，获得奖励$R_t$，环境获得动作$A_t$, 给出奖励 $R_t$，给出新的观察 $O_{t+1}$

在 RL 中， 我们在做 $\max_{\theta}E_{(s,a)\sim\rho^{\pi_0}}[r(s,a)]$

强化学习实际上可以看做是在优化数据集！

### RL 系统中的要素

- 历史(history): $H_t$, 观察、动作、奖励的序列

- 状态(state): $S_t, S_t=f(H_t)$, 用来确定接下来会发生的事情

- 策略(policy): 状态到动作的一个映射。
  
  - 确定性策略(deterministic policy), $a=\pi(s)$
  
  - 随机策略(stochastic policy), $\pi(a|s)=P(A_t=a|S_t=s)$

- 奖励(reward): $R_t$ 和 $r(s,a)$，一个用于定义强化学习目标的标量

- 模型(model): 用于模拟环境的行为。用于预测下一个状态和立即奖励。

强化学习的目标：$\max_{\pi}E_{\pi,\text{Env}}[\sum_{t=0}^T\gamma^tr(s_t,a_t)]$, 其中 $\gamma\in[0,1]$, 为一个衰减因子。也就是在 $T$ 步的情况下，效果还不错。 

为什么是指数？这样可以保证当两个序列函数值不同时，在序列最前面插入一个 $x$,结果依然保序。

价值：一个标量，用于定义长期来说什么是“好”的。

$Q_{\pi}(s,a)= r(s)+\gamma\sum_{s'\in S}P_{sa}(s')\sum_{a'\in A}\pi(a'|s')Q(s',a')$

一般而言，就是基于 $Q$ 改进 $\pi$，再重算 $Q$

### RL 的方法分类

- 基于价值

- 基于策略

- Actor-Critic

深度强化学习
