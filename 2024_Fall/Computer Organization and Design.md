# 计算机组成与设计(玉川整理)

## 第一章 计算机抽象及相关技术

存储容量的十进制术语和二进制术语的区别：KB, MB, GB 为十进制；KiB, MiB, GiB 为二进制。

计算机体系结构的七个“伟大思想”：

- 使用抽象(abstraction)简化设计

- 加速大概率事件

- 并行提高性能

- 流水线提高性能

- 预测提高性能

- 存储分层

- 冗余提高可靠性

计算机的经典部件：输入设备，输出设备，存储器，数据通路（运算器），控制器。后二部分合称处理器。

RAM名字的由来：访问RAM中各部分的数据，时间基本一致。内存即为DRAM，D表示dynamic；cache则为SRAM,即static.

存储器分为易失性存储器和非易失性存储器。前者称为主存储器或一级存储器。后者称为二级存储器。

Amdahl定律：$t = \frac{a}{k} + a'$，a为被加速部分，k为加速比，$a'$为未加速部分。说明性能优化与整体的关系。

摩尔定律：18个月到24个月，集成电路上的晶体管数量翻倍，价格下降为原来的一半。目前，摩尔定律放缓。

SRAM,DRAM,二级存储器构成了存储器层次(hierarchy of memory)中的三层。

在个人移动设备中，二级存储器常是闪存而非磁盘。

闪存体积小，可靠性好，能耗低，但是一个单元写入1e5至1e6次就可能损坏。

CPI：CPU时间周期数除以指令数。(cycle per instruction)

## 第二章 指令

指令系统(instruction set):指令的词汇表。

JAVA一开始是解释器。如今为了性能，会进行即时编译从而运行。

指令的设计原则1：简单源于规整。要好翻译。

算术运算指令

在RISC-V中，32位称为字，64位称为双字(doubleword)。

指令的设计原则2：更少则更快。指令的操作数如果过多，那么很有可能会增加时钟周期。因为电信号传播耗时增加了。

数据传输指令

计算机分为大端编址和小端编址。大端编址：存储内容的高位存储在高字节中。小端编址则相反。

寄存器比内存显然快得多。并且，内存数据必须先读出。因此寄存器明显有着更高的吞吐率。

立即数操作也可以减少存取指令。RSIC-V有着常零寄存器，方便我们进行操作。

二进制补码(two's complement)

这种存储方式利用的是算术上的溢出。考虑到这一点，我们可以将有符号数用统一的公式表示。即：

$$
x=(x_{31}\times (-2)^{31})+\sum_{i=0}^{30}(x_i\times 2^i)
$$

符号扩展的过程就是在数据左侧填充符号位，正确地表达目标数。有符号载入也是这个过程。

为什么不使用反码？因为计算过程中需要额外的一步去计算。虽然正数和负数是平衡的，但却存在正零和负零。

计算机中的指令表示：二进制机器码。称这种指令的设计为指令格式。在RSIC-V中，指令都为32位长。符合简单源于规整的原则。我们把RISCV指令的每一部分称为一个字段。

一般的RISCV指令组成是：funct(7)+rs2(5)+rs1(5)+funct(3)+rd(5)+opcode(7)

所加的每一项都是指令的一个字段。

设计原则3：优秀的设计需要适当的折中。

上所述为R型指令。考虑到某些指令需要更长的字段，从而有:

- I型：immediate(12)+rs1(5)+funct(3)+rd(5)+opcode(7)

- S型：immediate(11:5)+rs2(5)+rs1(5)+funct(3)+immediate(4:0)+opcode(7)

为什么S型这么奇怪？这是为了保证rs1和rs2在不同指令中处于相同位置，降低硬件设计的复杂性。其他字段亦是同理。

考虑到指令长度与寄存器数量有不可分割的关系，所以大部分架构寄存器数量为16或32个。

逻辑操作

在RSICV中没有取反操作。因为取反等价于和-1取异或。

c++中位字段的使用：

```cpp
//ref:https://blog.csdn.net/wendyWJGU/article/details/134880176

#include <iostream>
using namespace std;

#include <fstream>
struct Data1
{
    unsigned int bit0 : 1;
    unsigned int bit1 : 3;
    unsigned int bit2 : 6;
    unsigned int bit3 : 22;
};

int main()
{  
    unsigned int uSrcData = 0x12345678;

    cout << hex << "\n\nuSrcData = 0x" << uSrcData << endl;
    Data1 d2;
    d2.bit0 = uSrcData >> 31;
    d2.bit1 = (uSrcData >> 28);
    d2.bit2 = (uSrcData >> 22);
    d2.bit3 = (uSrcData);

    cout << hex << "bit0 = 0x" << d2.bit0 << endl;//0x0
    cout << hex << "bit1 = 0x" << d2.bit1 << endl;//0x1
    cout << hex << "bit2 = 0x" << d2.bit2 << endl;//0x8
    cout << hex << "bit3 = 0x" << d2.bit3 << endl;//0x345678
}
```

对位字段进行合适的操作，可以方便地取出无符号数中我们想要的内容。

分支地址表：在数据区存储多分支的起始地址。当遇到多分支结构时，在这部分进行寻址。这样可以减少分支语句数。

计算机硬件对过程的支持：比如ra(x1),a0-a7(x10-x17)这些寄存器。

栈指针sp(x2)

x5-x7及x28-x31为t0-t6

x8-x9及x18-x27为s0-s11

如果一个过程不调用其他的过程，那么我们称这个过程为叶子过程。如果在嵌套过程中，那么我们就需要将必要保存的寄存器进行压栈，调整sp.

x3(gp)的作用：为了简化对静态数据的访问，**部分**RSICV保留x3用作全局指针。

栈也会为局部变量分配空间。我们把保存有寄存器和存储有局部变量的段称为过程帧或者活动记录。有些编译器使用fp(x8)用作帧指针。指向帧的第一个字。如果对sp处理得好也许可以不用这一点。

栈从用户地址空间的高端开始。低端内存的第一部分为保留区。第二部分为代码区。第三部分为静态数据区。第四部分为堆区。也就是说，栈和堆是**相向**生长的，这样有利于对内存的高效利用。C调用malloc和free来分配空间或者释放空间。

RSICV中，栈空间从0x0000 0x3f ffff fff0开始向下；代码从0x0000 0000 0040 0000 开始向上。静态数据区从文本段末开始。

尾调用具有高效的迭代。

ASCII码对人类非常友好。但是java中，为了更有包容性，其默认采用unicode编码。其默认采用16位表示一个字符。riscv保持16字节栈空间对齐。

RSICV中的寻址模式，无非是立即数寻址，寄存器寻址，基址寻址（寄存器加常量）和PC相对寻址四种。条件分支只能到达附近的区域。

### 并行

如果并行执行的任务之间不相互同步，就存在数据竞争的风险。并行机制依赖于硬件提供的同步指令。

在互斥区中的数据，只有单个处理器可以操作。

RSICV中对并行的支持指令：lr.w（保留加载）和sc.w（条件存储）等。这两条指令要顺序使用。也就是先进行保留加载，再进行条件存储。如果保留加载的目标地址，在条件存储执行之前，其中内容发生了变化，那么就不会成功。sc.w有三个寄存器参数。第一个rd用于表征存储成功还是失败，值为1或0。rs1指定存储的内容，rs2指定地址。利用这两个指令，可以实现原子交换。这样的操作可以构建更多的原子操作。要想获得锁变量，我们也需要进行原子的操作。但是，释放的时候就随意了。

### 翻译

首先通过编译器。编译器的作用，就是把高级语言转换为汇编语言。

然后通过汇编器。汇编器的作用是对伪指令进行转换。转换为RISCV中的基本指令,并且生成二进制机器码，生成目标文件。在汇编器中，对符号表和指令位置进行了很好的维护，从而进行汇编。

接下来通过链接器。链接器的作用是将目标代码和目标库之间进行连接缝合。链接器会将代码和数据模块按照符号特征一起放入内存，重写数据和指令标签的地址，最后修正目标文件中的内部引用以及外部引用。从而生成可执行文件。可执行文件和目标文件的格式是相同的，但是目标文件中可能有未解析的引用。

在运行时，需要加载器进行工作。加载器会把可执行文件加载到内存中并启动之。

除去静态链接库，还有一种链接方式是使用动态链接库(DLL)。这是因为静态链接库可能很大，并且在库代码更新后，原程序并不会实时更新，需要重编译等操作。

起初的DLL，是运行一个动态的链接程序，使用额外的信息来查找相应的库并更新外部引用，但是它还是链接了所有可能的例程。DLL的延迟过程链接版本改良了这一点。当需要一个DLL中的程序时，程序会调用一个虚入口。它会加载一个表示目标例程的编号，然后到达动态链接器或者加载器。由其找到所需例程，并且修改虚入口之后的跳转地址。在后续对这个相同例程的调用时，操作系统会使用虚拟内存进行管理，之后不必再执行重映射操作。

曾经，指针自增比数组的效率更高，因为加法比乘法方便。但是现在的编译器可以帮我们做这些内容了。

ARM：最流行的嵌入式设备使用的架构。

ARM是另外一种与RISCV相类，但组成很不同的架构。它具有更多的寻址方式，并且有传统的四位条件码：负，零，进位，溢出。

ARM中的12位立即数：字段低8位零扩展为32位值，然后循环右移。移位次数是字段高四位指定的数字乘2。

循环右移：将最右侧的值右移至首位。

ARM支持保存一组寄存器。称为块加载和块存储等。

x86一直在不断更新。1985年发布了80386.但是通用寄存器只有8个。

x86存在一种很特殊的寻址方式，暂称之为比例下标寻址好了。即$基址+2^{比例}\times 下标，比例\in\{0,1,2,3\}$

x86的整数运算中存在数据传送指令，比如move, push, pop等指令。其亦存在字符串指令，包括字符串传送和字符串比较等。push和pop能够把数据在栈中进行读取。x86中甚至存在loop这种循环指令，也存在inc,dec等自增，自减指令。

RISCV中基本结构的剩余指令:slt, sltu, slti, sltiu, auipc.

auipc很合适于在动态链接库中使用。

Python 的解释器性质决定了性能不会比C 的编译器好。

## 算术运算

### 加减法

在计算机中，减法显然可以通过加法进行计算。实现加法和减法的硬件是ALU.

有的加法器存在**饱和**，即超出最大上限时，为最大值；低于最低下限时，为最小值。

#### 超前进位

关键无论输入何时改变，硬件总会并行执行。考虑$a+b$，则：

$$
c_{i+1}=(b_i\cdot c_i) +(a_i\cdot c_i) +(a_i\cdot b_1)
$$

这无非是在描述只要有两个是1，就得进位。上式也可以写成：

$$
c_{i+1}=(a_i+b_i)\cdot c_i+(a_i\cdot b_i)
$$

我们定义：

$$
g_i=a_i\cdot b_i\\
p_i=a_i+b_i
$$

从而，上式可以写成：

$$
c_{i+1}=g_i+p_i\cdot c_i
$$

g表示generate，p表示propagate.也就是说，只要$g_i=1$,那么一定会产生进位。如果$p_i=1$，那么一定可以把上一位的进位传递下去。

然后，我们可以把几个带有超前加法的位加法器捆在一起。把多组这样的捆绑捆在一起，可以得到一个更大的超前进位加法器。

### 乘法

最平凡的方式：被乘数用双倍空间存，乘数用正常空间存。每次周期乘法结束后，被乘数左移一位，乘数右移一位。每次仅考察乘数的最后一位。这样，只需要把被乘数和积之间作运算就可以了。

为了对硬件进行优化，我们可以充分利用双倍空间的积寄存器。我们可以把乘数储存在积寄存器中，每次同样只操作乘数的最后一位，结果加在积寄存器的高位上。每次对积寄存器进行右移。

带符号乘法是平凡的。我们只需要把符号先记录下来就好了。

还有一种方式，是我们可以考虑使用一堆加法器，组成一棵树的形状来加速这个过程。

### 除法

最简单的除法，那就是移位加减法。一开始把余数寄存器初始化为被除数。这很符合人类直觉，不复赘述。

改进版本是类似的。只不过能够优化下硬件罢了。就是把余数寄存器和商寄存器存储在了一起。

有符号的除法是类似的。

快速除法一般采用查表法来预测。通过余数后六位和除数前四位预测这一点。

### 浮点数运算

浮点数运算采取的是科学计数法。整数部分为1.形如：

$$
1.xxxxx...\times 2^{yyyy...}
$$

32位浮点数一位符号，8位指数，23位尾数。浮点数的特点是会产生上溢和下溢。对于64位浮点数，指数为11位，尾数为52位。

有的时候，发生上溢出或者下溢出会产生中断。

### IEEE 754的浮点数标准

如果一个浮点数的指数为0，但是尾数不是0，那么其表示的对象是正负非规格化的浮点数。如果指数位全为1，尾数位为0，那么我们用它表示正负无穷。如果指数位全为1，尾数不为0那么我们用它来表示非数(NaN)。

### 浮点数加法

把浮点数指数较小的数字的指数转化为和指数较大的浮点数相同。即将尾数进行右移。然后，将二者进行有效位数相加。然后，进行规格化。

### 浮点数乘法

平凡地，正如直觉那样，先把两个数字的指数相加。然后，将两个数字的尾数部分进行相乘，得到计算的结果，然后得到规格化的表示。

### 浮点数运算中的精确算术

在运算的过程中，会额外保留两个bit，这样在舍入的时候会更加精确。

### 子字并行

一个128位的加法器，也可以视作16个8位加法器。这种对短向量分割后，进行并行处理的方式可以提高处理的速度。  

### 一些常见问题

- 右移不等价于除以2的整数次幂。

- 浮点数加法不满足结合律。因为溢出！

## 处理器

### 时钟同步方法

采用沿边触发的时钟。这样，可以保证每个部件对状态的读取和修改具有同时性。

如果一个部件，并不是每一个时钟周期都会做出反应，那么我们需要额外采取一个控制信号，来控制信号的更新。我们用有效(asserted)来表示控制信号的高电平。反之，则表示低电平。

### 数据通路

数据通路单元：用于操作或者保存处理器中数据的单元。RISCV中，包括指令存储器、数据存储器、寄存器堆、ALU、加法器。

一般而言，寄存器堆需要有两个读端口（因为有不少指令一次要读两个寄存器中的内容）和一个写端口。这几个端口的导线都是5位的。此外，还有三根对应的数据线。它们都是32位的，用于传输数据。同时，寄存器堆需要有一个写控制信号。从而保证只在我们需要的上升（下降）沿进行写操作。

立即数生成单元，可以把指令中的一些立即数进行符号扩展，然后用于使用。这在存取指令中很常见。

在数据存储单元中，我们有一个地址输入，写数据输入和读数据输出。该单元，不可以同时执行两种指令。并且，它既需要一个读信号，也需要一个写信号。

分支指令判断：从寄存器堆中读取内容。判断分支是否成立。然后，选择对PC的操作方式。

可以通过多路选择器，将一些较为相近的结构进行复用。

小型的ALU控制单元：将机器码翻译成需要ALU进行的操作的译码。

主控制单元：首先，由于RISCV中的机器码具有一定的规律性，所以在处理时，数据通路得到了一定的简化。从而对硬件进行简化，甚至一定程度上加快了运行的速度。

指令格式固定的好处在于，可以直接把对应位置的编码强加到目标位置上，而无需考虑opcode。opcode起到的是控制线的作用。也就是说，将opcode加到主控单元中，然后由主控单元决定对每一个单元的控制。

### 数据通路的操作

#### R型指令的数据通路操作

1. 取出指令，PC自增。

2. 从寄存器中读出内容，主控制单元计算控制信号。

3. 根据操作码确定ALU功能，对读出的数据进行操作。

4. 将ALU结果写入目标的寄存器。

### 流水线

流水线不能提高单步骤的速度。但是，流水线可以提高系统的吞吐率，从而减少完成整个任务的时间。

RISCV指令执行通常包含五个步骤：取指令、读寄存器并译码、执行操作或者计算地址、访问数据存储器中的操作数、将结果写入寄存器。

现代x86架构中，x86指令的长度不同，为了方便进行处理，所以会将这样的指令转化为像RSICV中那样的简单指令进行处理。

理想状况下，流水线的加速应该是$\frac{ideal\ CPI}{number\ of\ pipe\ stages}$。但是这一般是达不到的。

我们记流水线下的加速为：

$$
\text{speedup}=\frac{(ideal\ CPI)\cdot (number\ of\ pipe\ stages)}{(ideal\ CPI)+(pipeline\ stall\ CPI)}
\cdot \frac{cycle_{unpiped}}{cycle_{piped}}
$$

前面一项，为是否在流水线下的CPI的比值。后一项，为时钟周期的比值。

### 流水线冒险

#### 结构冒险

硬件上的不支持。但由于RSICV是面向流水线设计的，所以这一点较为容易避免。

#### 数据冒险

一个步骤必须等待另一个步骤完成的情况。为了解决这个问题，我们采用称为前递(forwarding)或者旁路(bypassing)的方式来优化这一点。也就是说，在数据还没有被显式地写入寄存器中时，就将其传给目标指令。

对如load等指令而言，会造成流水线上更长时间的停顿，也就是为了处理载入-使用型数据冒险导致的情况。

#### 控制冒险

需要根据指令的结果做出决定。但是决定是否跳转的指令正在运行中。因此，我们一般采用预测的方式来处理条件分支。因为停顿这种解决方式太慢了。还有一种方式来解决这个问题，我们称之为延迟决定(delayed decision)。也就是说，我们把一些与分支无关的指令，放到计算跳转条件值之后进行。

### 流水线数据通路和控制

#### 指令的五个阶段

1. IF:取指令

2. ID:指令译码和读寄存器堆

3. EX:执行或计算地址

4. MEM:存储器访问

5. WB:写回

上述的五个过程是一次执行的。但是，存在两种反向的数据流动，可能会对后续指令的执行产生影响。它们是：

- 写回过程中，会对寄存器堆产生影响。

- 下一PC值会根据运算结果进行选择。

利用上升沿和下降沿，保持正确的时序。

可以前半周期写入，后半周期读入。

也就是说，我们可以把每一部分进行分离，后半周期读入，前半周期写入。每一个逻辑部件之间实现分离。

#### 流水线控制

类似之前的方式。利用不同的opcode，对不同的逻辑部件进行控制。比如说，我们考虑是否对内存进行读取，ALU将要执行的指令到底是个R指令还是个I指令，PC应当要变为何值等问题。这里我们可以使用一个MUX来解决这个问题。在这里，我们可以使用一个MUX来决定最终读取的内容。

### 前递与停顿

冒险的两对条件：

- 运算结果储存对象为运算数之一。

- 访存结果储存对象为运算数之一。

为了解决这些问题，我们可以使用前递单元来解决这个问题。也就是说，将运算所得结果通过前递单元递回，通过MUX决定最终采取哪一个数值。在前递单元中，我们可以选择三种状态：由寄存器堆取的内容；由ALU运算结果取得内容，由访存取得的内容。

#### 数据冒险与停顿

一种常见的解决方法，是通过插入一条空指令来解决这个问题。

#### 三种常见的数据依赖

- 写后读（WAR）

- 写后写（WAW）

- 读后写（RAW）

WAR是真实的数据依赖。剩下两个则是虚假的数据依赖。对于后两者，我们可以使用寄存器重命名的方式来解决。

### 控制冒险

解决分支跳转的方式有如下几种：

- 假设分支不发生

- 缩短分支延迟

- 使用动态分支预测器

第二点，是通过汇编代码移动的方式实现的。

对于分支预测器，比较常见的实现方式是分支预测缓存，或者分支历史表。

目前，比较好的分支预测器是相关预测器，也就是同时利用该分支以及近期全局分支来进行预测。另一种比较好的预测器是锦标赛分支预测器。也就是一个分支给出多个预测，然后通过一定的选择机制来进行挑选。

### 例外和中断

例外，指的是意外的控制流变化，指的是无需区分来源于处理器内还是处理器外的控制流变化；中断，则一定是处理器外部引发的。

对于例外的处理，一般会在系统例外程序计数器中保存发生例外的指令地址(SEPC)，然后把控制权移交给操作系统。为了说明到底是什么原因，我们需要把原因存入系统例外原因寄存器(SCAUSE)。或者，采用向量式中断方式。也就是说，在向量式中断内存区域的基础上，再加上一个对应的例外值，将该值作为例外的表示。

我们可以把例外处理看做是一种特殊的控制冒险。

在遇到分支预测失败的情况下，我们要清除取指阶段的指令和译码阶段的指令。并且，我们应当给出新的信号，控制EX、WB等阶段输出恒为0.

### 指令间的并行性(ILP,instruction-level parallelism)

#### 数据依赖的种类

- 真数据依赖

- 名称依赖

- 控制依赖

如果指令$j$会用到指令$i$中的指令，或者指令$j$会依赖的指令$k$依赖于指令$i$，那么我们称指令$j$依赖于指令$i$.

真数据依赖，指的是确实完全不能交换顺序的两条指令。它们之间具有数据流动。

名称依赖，指的是两条指令使用了相同的寄存器名称或者地址等。但是，二者之间并没有任何的数据流动关系。比如读后写就是一种名称依赖，读后写又称为反依赖。同样的，还有写后写也是一种名称依赖，称为输出依赖。

三种数据冒险：

- 写后写

- 写后读

- 读后写

控制依赖：保证分支前语句不可出现于分支中。分支中语句不可外提到分支前。

我们也需要保证数据流中数据的正确性。对数据流进行分析，我们也可以找出死代码。

#### 流水线调度和循环展开

对于独立的指令，我们可以将它们进行调度，从而减少停顿的可能。

如果一个循环中，所有的操作都是并行的，那么平凡地，我们可以把这个循环展开，使之能够并行地执行。

但是，在指令调度过程中可能产生寄存器紧缺的副作用。

#### 高级分支预测

##### 相关分支预测器

不仅和当前分支有关的预测器，我们称之为相关预测器。

$(m,n)$预测器利用的是最近$m$个分支的行为，总共有$2^m$个分支预测器，每一个分支预测器都是$n$位的。

这在硬件上是易于实现的。我们可以做一个$m$位的移位寄存器来记录最近$m$个分支的行为。我们把分支地址的低四位和这$m$位拼接在一起，作为一个索引。这样的六位索引可以用来寻址

##### 竞争预测器

在全局预测器和局部预测器之间竞争。由分支地址决定使用的局部预测器，由分支历史决定选择的全局预测器。由分支地址和分支历史共同决定选择的是哪一种寄存器。

##### 带标签的混合预测器

有多个不同的预测表，每一个表对应着不同的分支历史数目。它们共同作用，得到结果。

#### 动态调度

通过乱序执行来减少停顿。这样会引入一个问题：如果产生异常，异常发生时处理器状态可能与按照顺序执行时的情况不一致，导致非精确异常的产生。

#### Tomasulo

寄存器重命名可以解决WAW和WAR的问题。在这里，我们使用保留站来进行寄存器重命名。

Tomasulo中指令的三个过程：发射，执行，写回。

保留站中一条指令的组成部分：busy,op,q1,q2,v1,v2,a

Tomasulo的优势

- 一条指令计算完成后，可以同时使多条指令处于可执行状态。

- 消除了除真数据冒险外的冒险情况。

在对内存进行读写的时候，我们需要检查载入指令和读取指令的地址是否重合。如果重合，那么就不可以这么做。

Tomasulo的关键是乱序执行，顺序提交。因此，指令在最后会进入重排序缓冲区结构。在这里，指令的结果会被顺序提交。

指令执行的四个步骤：

- 发射。要求ROB和RS中都有空位。

- 执行。如果有操作数不可用，那么在等待计算的过程中需要监视CDB。在这里，检查RAW冒险。在这一步，存储只需要计算目标地址即可。只有在commit的时候，我们才需要真正地把这个地址写入。但是对于载入而言，没有办法，我们必须得把这个数据真正地读出来。

- 写结果。对于每个单元而言，如果结果计算完成，那么就将它写到总线CDB上。每一个保留站都需要监听CDB上的信息，然后再更新对应的ROB中的信息。

- 提交。提交有三种类型。对于存储提交而言，我们需要将对应的数据切实地写回到存储器中。对于预测错误的提交而言，我们需要清空一切正在执行的指令。然后重新恢复原状。对于其他的提交而言，我们只需要正常提交即可。

解决乱序执行过程中，存储器可能发生的数据冒险的方法：

- 如果存储的地址与读取的地址相同，那么不允许读取指令开始第二阶段（读取）。

- 计算载入指令的有效地址时，保持之前存储指令的顺序。

#### 多发射

多发射处理器主要有三种类型：

- 静态调度超标量处理器

- 动态调度超标量处理器

- VLIW（超长指令字）处理器

对于两种超标量处理器而言，每周期发射的指令数目是不固定的。但是，VLIW每周期发射的指令数是固定的。VLIW和静态调度超标量处理器都是依靠编译器进行的工作。静态调度超标量处理器常用在嵌入式（比如ARM架构）中。动态调度超标量处理器感觉是当下微机处理器的主流。VLIW则常见于信号处理中。

##### VLIW

VLIW的思想就是想要同时利用不同的部件，从而提高CPU的工作效率。因此，其会将可以同时发射的语句包装成一个大语句，一起发射出去。这样存在的问题是：为了提高并行度，不可避免的，需要进行循环展开等工作，这样会极大地提高代码的长度。此外，由于一条指令中，很可能有一部分处于空置状态，所以，代码的冗余程度也进一步加大。

##### 动态调度，多发射，推测

动态调度的Tomasulo架构，需要考虑如何发射，才能够保证原来代码的执行顺序正确。并且，各种总线的宽度也应该被拓宽。

有两种方式解决多发射的发射问题。第一种，是在时钟上升沿和下降沿各发射一次，但是这样每个周期至多执行两条指令；还有一种，则是采用额外的部件来解决这个问题。

简单而言，这个问题的解决方式分三步走：

- 检查有多少条指令可能被发射，为所有的这些指令预留保留站中的位置。

- 检查发射的指令之间相互的依赖关系。

- 更新依赖表项。

#### 关于指令交付和带宽

##### 分支目标缓冲区

简单的说，这一部分提供了一个hashmap，其为当前PC到预测PC和预测分支结果的一个映射。每次取指到当前PC，可以查询其是否在缓冲区中。如果在，那么我们可以根据预测结果去进行更新PC，并且去取指令。如果不在，那么我们可以判断当前PC对应的语句是否为分支指令。如果是，我们则将其添加到缓冲区中。

#### 专用分支预测器

过程返回预测。也就是说，额外开启一个栈，来存储call时存放的地址。每次有return时，取出栈顶，然后弹栈。以栈顶元素作为返回的预测值。

#### 集成取指单元

这是为了满足多发射处理器对指令的需求。在这里，分支预测器被视作是集成取指单元的一部分。这一单元需要解决三个问题：

1. 集成分支预测。

2. 指令预取。

3. 指令存储器访问和缓存。

#### 推测

另一种实现寄存器重命名的方式：把结果先暂存在逻辑寄存器中。之后再搬运到物理寄存器中。这样做的好处，在于没有目标寄存器的指令，能够不占用逻辑寄存器。也就是说，在指令发射的同时，我们就把所有的指令进行了一次重写，直接对指令中使用和定义的寄存器进行重命名。

推测是具有代价的。尽管，对于简单指令而言，推测可以减少流水线停顿的可能。但是，如果遇到cache-miss等情况，一旦推测错误，它的时间成本是巨大的。因此，常见的做法是对其进行等待，直到该指令的执行不再具有推测性为止。

我们也可以考虑同时对多个分支进行推测。这样做在分支频率很高的时候是合理的。但目前来看，这样做的性价比并不是很高。

我们认为，推测会提高能耗。这是预测错误导致的问题。因此，目前的解决思路是想办法减少错误的推测，或者只在预测准确度高的地方进行预测。

地址别名预测可以提高载入、存储指令的效率。但是，目前还没有什么诱人的成果。

#### 多线程

多个线程共享单个处理器的功能单元。线程切换的开销远小于进程切换。

目前主要有三种方式来实现多线程。

- 细粒度多线程(fine-grained multithreading)

- 粗粒度多线程(coarse-grained multithreading)

- 同时多线程(simultaneous multithreading, SMT)

细粒度多线程，指的是每个时钟周期都会发生线程间的切换。所有停顿的线程都会被跳过。因此，当一个线程停顿时，我们可以执行另一个线程的指令。这种方式是减缓了单条指令的执行速度，但是提高了多线程的吞吐量。

粗粒度多线程，指的是在发生成本较高的停顿时，才发生线程切换。比如cache-miss.

同时多线程，首先也可以隐藏长延迟事件造成的延迟。但是，其可以通过寄存器的重命名和动态调度来执行不同线程的多条指令。

## 存储器

### 局部性原理

- 时间局部性：刚刚用过的某个数据项可能会在不久后被再次访问。

- 空间局部性：刚刚被访问的某个数据项，和它相邻的数据项也可能会在不久后被访问。

因此，根据局部性原理，我们可以用它来构建层次化的存储结构。

越快的存储器越靠近处理器，它的容量也越小，价格也越高。最后一层的存储器，在微机中一般是磁盘。它里面放有**所有**数据。

在层次化存储中，数据是一层一层流动的。数据要从第三层转移到第一层，必须先到第二层。并且，在相邻两层之间进行信息交换，是有一定的最小单元的。我们称之为块，或者行。

我们称数据在本层存储中找到为"hit"，没有找到为"miss"，命中率(hit rate)就是hit占访问该层总次数的比率。失效率(miss rate)则自然地，与之相反。 

**命中时间**：访问某一层数据所需要的时间。包括判断是否命中。

**失效损失**：把数据块从下层存储器复制到某层所需的存储器，并且将其返回给处理器所需的时间。

### 存储技术

目前，主流的四种层次化存储技术为：

- 静态随机访问存储(SRAM)(0.5-2.5ns)

- 动态随机访问存储(DRAM)(50-70ns)

- 闪存(flash memory)(5000-50000ns)

- 磁盘(magnetic disk)(5000000-20000000ns)

四种类型从上到下，它们的访问时间逐步增大，但是成本也随之下降。

#### SRAM

SRAM是一种简单的集成电路。对于任意位置的数据，SRAM访问时间固定。这是因为它不需要定期刷新电路，而是用6-8个晶体管存储一个bit。它的访问时间和两次存储访问的间隔周期时间接近。因此，在SRAM中，只要不断电，数据会被一直保存。

曾经，SRAM芯片都是作为独立的高速缓存芯片。目前，这些高速缓存都已经被集成到了处理器上。

#### DRAM

DRAM中，用电容来存储数据，并且用单个晶体管来访问电容中存储的电荷。在DRAM中，一个晶体管就存储着一个bit，所以它的存储密度更高。但也正因为这一点，所以DRAM中不可以长期保持数据，需要进行周期性的刷新，因为电容会缓慢地进行放电。因此，我们称之为动态。刷新的做法是：一个读周期，然后跟一个写周期。

多个这样的晶体管-电容结构堆在一起，就组成了cell阵列。在cell阵列中，cell排布成正方形。每一行共享一根word line，控制晶体管开关的接通与否；每一列共享一根bit line，用于读取电容中的内容。

然后，将这些阵列封装在一起，就得到了bank。它们共享同一组word line和bit line。

在bank中，我们将其分为一系列row。在当前的DRAM中，row也会被缓冲。目前，为了更加高效，我们会为每个bank准备对应的行缓冲。

此外，为了优化和处理器的接口，有一种称为SDRAM（同步DRAM）的结构，从而消除了内存和处理器之间的同步问题。这样做有利于突发传输(burst transfer)。也就是说，不再每个周期均指定待读取的地址，而是在某个周期开始时指定起始地址，突发类型，突发长度。然后，在每个周期时均返回其需要的数据。这是由周期的同步性保证的。

更快的DRAM：双倍数据传输率(DDR)SDRAM.也就是说，在时钟周期上升沿和下降沿，其都会传输一次数据。

服务器的存储通常会集成在电路板上。这被称作双列直插式内存模块(Dual Inline Memory Modules, DIMM)。DIMM中可以有多个DRAM芯片，我们称这个芯片的集合为rank。

#### 闪存

闪存是一种电可擦除的，可编程的只读存储器(Electrically Erasable Programmable Read-Only Memory, EEPROM)。闪存的写操作会对闪存本身造成磨损。因此，闪存中存在一个控制器，来将发生多次写的数据块重新映射到写的较少的数据块中，使得写操作尽量分散。这种技术被称作耗损均衡。

闪存比较坚固，更加适合于移动设备。

#### 磁盘

磁盘由盘片组成，它们每秒旋转约5400-15000周。它们的每一面上都由磁性材料进行覆盖。在盘面的上方，有一个被称为读写头的小型电磁线圈。磁盘表面可以分成上万个同心圆，我们称之为磁道(track)。磁道按顺序，划分为上千个可以保存信息的扇区(sector,磁道上的一段)。每个盘面都配有一个磁头，它们互相连接，并且一起移动。每个磁头都可以读取每个盘面的每一条磁道。柱面(cylinder)表示某磁头在某一点处可以访问到的所有磁道的集合。

操作系统对磁盘数据的访问如下：

- 把磁头定位到正确的磁道上方。这称为寻道(seek)。所需的时间称为寻道时间(seek time).

- 然后，我们需要等待所需扇区旋转到读写磁头下。这段时间被称为旋转延时(rotational delay).

- 然后，我们要进行数据的传输。这一部分时间为(transfer time).

磁盘为了提高性能，也会进行一定的缓冲。

磁盘没有写损耗。

#### cache基础

cache的使用原理：如果处理器需要一个数据，会在cache中进行查找。如果cache中不存在这个元素，那么会产生一次cache miss。随后，会在内存中对该数据进行读取，并将这个数据读取到cache中。  

##### 直接映射cache

比较常见的直接映射cache，就是采用$块地址\text{ mod }cache_{size}$进行标记。但是，这样很明显，不同的数据块可能会存储于同一个cache位置中。因此，为了解决这个问题，直接映射cache中，每一个存储数据块上需要存储一组标签，其保存了必要的地址信息。标签只需要存储块的高位地址就足够了。同时，为了验证存储的有效性，我们也需要添加一个有效位，用以表示这个数据是否有效。

cache初始化为空，所有的有效位都会被标记成无效。

对于所有要去访问的地址，我们可以将其分为标签字段和索引字段和字节偏移量。索引字段，用于查找cache中对应的数据块。标签字段，用来和cache中的标签位进行比较。字节偏移量，用来取出对应的数据。如果有cache中有$2^n$个数据块，那么我们就需要$n$位作为索引字段。如果数据块的大小为$2^m$个word，那么我们就需要在数据块中使用$m$位来确定单字，并且用最后两位来确定word(4 bytes)中的字节偏移量。容量更大的块，cache的失效率会降低。但是，这样一来，对空间局部性的利用也下降（因为失效损失会增大），导致cache失效所带来的好处有所减少。同时，如果在cache大小不变的情况下，cache快大小过大，失效率也会提高。

为了尽量减少失效损失所带来的影响，所以我们需要想办法解决这个问题。

- 提早重启(early restart)：只要数据块中我们所需的数据返回了，就立刻继续执行。然后，我们再慢慢地把数据写入cache。

- 关键字先行(critical word first)：让所需的数据先传输到cache中，再慢慢传输数据块中的剩余数据。效果略好于提早重启，但也有着相同的问题。

##### cache失效处理

如果cache命中，那么我们返回或是修改数据就好了。没有必要去做更多工作。但如果cache失效了，那么我们必须要做更多的工作。cache失效，会引发流水线一定的停顿。失效后的处理，简单地说，就是把数据从内存读到cache中，然后，再次执行cache中的读写操作。这一次会在cache中命中。然后继续处理。

##### cache写操作

如果每次对cache中的数据进行修改时，我们不将其直接写到主存中，那么cache和主存则是不一致的。为了保持cache和主存一致，一种简单的方式是在cache修改后，直接将数据写回内存。这种方式被称为写穿透(write-through)。如果发生了写失效，按照写穿透的思路，我们也从主存中取出数据，将其写入cache，然后再将其写回。这种方式简单，但是效率比较低。

一种稍好的解决方式是使用写缓冲(write buffer)，其中保存着等待写回主存中的数据。也就是说，数据在写入cache的同时也同时写在缓冲中。缓冲中慢慢地将数据写入主存。如果缓冲区满了，那么处理器也不得不停顿，直到缓冲区中重新出现空位。

还有一种性能更好的策略，那就是写返回(write-back)。更新的数据只会被写在cache中。只有当数据被替换出cache时，才会被写到下一级存储之中。但是，这种策略的实现也会复杂不少。

##### 写失效和写返回的处理

在写穿透中，cache的实现主要有这两种：

- 写分配：把数据从内存取到cache中，改写数据块中的对应数据。

- 写不分配：直接在内存中改写对应的数据。

#### cache性能

$$
CPU时间=(CPU执行周期数+等待存储访问的时钟周期数)\\ \times 时钟周期\\
读操作带来的停顿周期数=\frac{读操作数目}{程序}\times读失效率\times 读失效代价\\
\\
写操作带来的停顿周期数=\frac{写操作数目}{程序}\times写失效率\times 写失效代价\\
+写缓冲满时的停顿周期\\
等待存储访问的时钟周期数=\frac{指令数目}{程序}\times\frac{失效次数}{指令数目}\times 写失效代价\\
+写缓冲满时的停顿周期
$$

平均存储访问时间(AMAT)用来衡量cache的设计。定义为：

$$
AMAT=命中时间+失效率\times失效代价
$$

#### 更灵活的替换策略

如果采用直接映射法，那么数据块在cache中只能有一个对应的位置。为了降低失效率，我们需要尝试更多的方式。

##### 全相联

主存中的某个数据块，和cache中的任何一个表项都可能有关联。这样做的问题在于增大了查询cache是否命中的时间。如果cache容量较小，这样做问题是不大的。

##### 组相联

组相联是一种相对折中的方式。主存中的每个数据块，会通过之前的类似于索引的方式，存放到对应的组中。但是，在组内，数据块的存放方式是任意的。如果一个组中可以存放n个数据块，那么我们称这是一个n路相联cache。因此，为了查找一个数据块，我们必须检查某个组中所有元素的标签。如果相联度越高，那么失效率会降低，但是命中时间会增长。因此，我们的主要目标就是要找到失效率和命中时间的平衡。

##### 如何查找数据块

在多路组相联中，我们需要n个比较器，以及一个mux。所以比较常见的，n一般为2的整数次幂。

##### 如何替换

最常用的策略是LRU.被替换的数据块是最长时间未被使用的块。相联度提高，那么标签的长度也要增加。这是因为索引的长度缩短了。

##### 多级cache

目前，二级cache一般和一级cache一起封装在芯片中。如果一级cache失效，那么就会尝试在二级cache中寻找。如果成功找到，那么一级cache的失效代价就是二级cache的访问时间。

多级cache可以进行更加合适的分工。通常而言，一级cache会相对而言比较小，并且使用较小的数据块容量，降低失效的代价。二级cache则会相对而言大不少。这是因为我们希望降低一级cache的访问时间，使得这更加高效；降低二级cache的失效率，使得相对而言失效影响没有那么大。

为了提升cache的利用率，在软件层面可以进行一定的分块，针对数据块来进行操作，提高程序的时间局部性，从而减少cache失效的影响。

在多级cache中，我们定义全局失效率，为在所有层级上，cache均失效的情况所占的比例。我们定义某级cache的局部失效率，为在该级上cache访问失效数目所占的比例。

对于乱序处理器而言，性能更加复杂。因为cache失效不意味着马上就会停顿。因此，我们采取平均每条指令失效次数来刻画cache的性能。公式为：

$$
\frac{存储停顿周期数}{指令}=\frac{失效次数}{指令}\times(所有失效代价-重叠的失效代价)
$$

#### 可靠性

层次化存储必须保证完全的可靠性。不然，这是没有意义的。

##### 失效的定义

实际上，系统在两种有需求的服务状态之间交换。

- 服务完成：交付服务与需求相符。

- 服务中断：交付服务与需求不同。

由完成到中断的过程，就是失效；由中断到完成的过程，就是恢复。

对于可靠性的度量，我们一般采用MTTF和AFR两种方式。前者，称为平均无故障时间，后者，称为年度失效率。指的是给定MTTF的情况下，一年内预期的器件失效比例。

MTTR称为平均修复时间，用于衡量服务中断的时间。我们定义，平均失效间隔(MTBF)为MTTF+MTTR，从而我们定义可用性为：$\frac{MTTF}{MTBF}$对于5个9的可用性，意味着每年需要5分钟进行修复。

一般而言，有三种提高MTTF的方法：

- 故障避免技术（不要坏）

- 故障容忍技术（容错，坏了也不要紧）

- 故障预测技术（发现马上要坏，故障前替换）

##### 汉明码

定义：两个等长的二进制序列中，它们的汉明距离为对应位不同的位数有多少个。如果所有的编码中，汉明距离最小是2，那么如果发生了一位的错误，我们就可以检查出来，并且予以纠正。这就是汉明码的思路：奇偶校验码。汉明码的做法如下：

- 从左至右编号，1-base。

- 把编号为2的自然数次幂的位留为奇偶校验位。

- 第$2^i$校验第$i$位的情况。即校验将每一位编号转写为二进制后，该位为1的位数的奇偶性。

- 把奇偶检验为正确地进行设置。

这样，每一个数据位都保证了至少有两个校验码进行校验。

如果我们为了追求更高的容错率，我们可以通过将纠错码转变为2位的方式，把最小汉明距离转变为4，从而可以检测出2位的错误，以及纠正1位错误。这种方法的实现是简单的，也就是添加一位，进行整个序列的奇偶校验。如果两部分均通过，说明没问题；如果都不通过，说明有1为错；如果前者不通过，那么说明有2位错；如果后者不通过，说明后者出错。

纠正1位错，检查2位错的技术被称为(SEC/DED)技术，校验码的位数应当为满足$p\ge\log_2(p+d+1)$

#### 虚拟机

虚拟机最宽泛的定义，是指包括所有的基本仿真方法，它们提供标准的软件接口。

我们把用于支持虚拟机的软件称为虚拟机监视器(VMM)。运行虚拟机的底层硬件平台被称为主机(host)，资源可以被客户端(guest)虚拟机共享。VMM用来决定如何把虚拟的资源映射到物理的资源。

虚拟机具有不少有点，它可以提供保护功能，管理软件，管理硬件等。

处理器虚拟化的开销取决于工作负载有多大。用户级处理器绑定的程序虚拟化开销几乎没有，因为操作系统很少被调用，一切以本地速度工作。IO密集型工作负载通常也是操作系统密集型的。因为系统调用很多，虚拟化开销会比较大。但如果工作瓶颈是IO，那问题就没那么大了。

VMM需要确保客户系统只和虚拟资源交互。

#### 虚拟存储

虚拟存储，指的是通过磁盘实现的辅助存储来充当cache.

虚拟内存产生的动机主要有两个：

- 允许多个程序之间高效安全地共享内存。

- 消除主存容量对程序设计产生的影响。

但是，共享内存的虚拟机，它们之间的关系可能会动态变换。所以，我们希望每一个程序编译到自身的地址空间中，也就是只有该程序才可以访问的存储位置。虚拟存储实现了把程序地址到物理地址的转换，这样也实现了各程序地址空间之间的保护。

另外，针对第二点好处，如果没有虚拟存储的话，那么程序员必须手动控制数据的载入，这是非常麻烦的。虚拟存储可以自动管理由主存和辅助存储代表的两级存储结构。

虚拟存储中，虚拟存储块称为页，虚拟存储失效被称之为缺页失效。虚拟存储中，处理器会产生一个虚拟地址，其通过软硬件转换，变成了一个可以访问主存的物理地址。这个过程被称为地址转换。

虚拟内存会通过重定位简化执行时的程序载入。重定位就是在将虚拟地址映射到不同的物理地址。

在虚拟内存中，每一个虚拟页对应一个物理页。但是，物理页可能在主存上，也可能在磁盘上。一个物理页可以对应**多个**虚拟页，从而便于程序之间进行数据的共享。

在虚拟存储中，数据被划分为了虚拟页号和页内偏移两部分。首先，虚拟页号会被转化为物理页号。它就是原来物理地址中的高位部分。但是，页内偏移不会做出改变。它就是物理地址的低位部分。

磁盘的缺页失效，代价是十分高昂的，可能需要上百万个时钟周期。因此，在设计虚拟存储系统时有几个决策：

- 页应该足够长来分摊长访问时间

- 尽量采用能降低缺页失效率的组织架构，如全相联等方式。

- 使用更好的算法来放置页数据。

- 对于虚拟存储而言，写穿透策略自然是不合适的。因为写回磁盘时间耗费巨大。虚拟存储一般采用写返回策略。

除了页式存储以外，还有一种可变长度块的机制，我们称之为段式存储。类似的，地址由段号和段内偏移两部分组成。

##### 页的存放和查找

在虚拟存储中，我们通过页表来索引对应的物理页号。页表用虚拟地址中的页号作为键值，从而映射到对应的物理页号。每个程序都有着自己的页表。硬件中，包含一个指向页表首地址的寄存器，称之为页表寄存器。在表项中，还有一位失效位，用来表明数据是否在主存中。

##### 缺页失效

如果虚拟页的有效位是无效，那么就会发生缺页失效，并且从下一级存储中进行读取。问题在于，如果我们一开始没有进行合适的初始化，那么失效的表项，对应的地址就会不知道是存在什么地方的了。所以，在创建进程时，操作系统一般会为所有页面在闪存或者磁盘上创建空间。我们把这个空间称为交换区。

操作系统还需要跟踪记录每个物理地址是哪些进程和虚拟地址在使用。如果发现内存中所有页都在使用，那么操作系统必须选择一页将其替换入下一级内存。这一步一般采用LRU.一般而言，我们采用近似的LRU算法，从而保证这一步的开销不会太高。也就是说，我们添加了一个引用位，被使用则置位。一段时间后全部清零。

##### 大虚拟地址空间

如果页表特别大，我们该怎么处理？主要有两个角度来解决问题：减少存放页表**所需**的最大空间；减少**用于**存储页表的存储空间。

- 保留界限寄存器。只允许地址符合要求的页表被添加入表项中。

- 使用两个页表。一个维护栈空间，一个维护堆空间。复合上界限寄存器，从而高效地进行维护。

- 反向页表。在这种情况下，我们可以对虚拟地址进行哈希函数来减少内存占用。但是，为了解决碰撞的问题，我们把相同哈希值的表项用链表的形式进行串联。并且在表项中记录pid，虚拟页号等信息来解决冲突。

- 将页表进一步分页。也就是把页表本身也保存在虚拟地址的空间中。

- 采用多级页表的方式，减少页表存储的总量。

##### 写操作

因为对磁盘读写的开销很大，所以一般而言，我们总是采取写回策略。并且，考虑到磁盘寻道的时间远大于数据传输的开销，所以一般我们总是一次读写一整页。在页表中，我们会添加一个脏位，用来表示这个数据块是否被写入过。如果有，我们在将其替换出时才需要写回。

##### TLB

TLB是一种特殊的地址转换cache，我们称之为TLB(快表)。TLB中保存了一系列物理页的位置。由于在查询时，我们会优先搜索TLB，所以其上也需要保留有效位、脏位、失效位等。当TLB中发生访问失效时，我们必须要确定这到底是缺页失效还是仅仅是TLB失效。一般而言，TLB的失效度也是比较低的。

##### 虚拟存储、TLB和cache的集成

我们从TLB失效、页表失效、cache失效三个角度来考察每种情况是否可能发生。cache命中，但剩下两者失效是不可能的。因为如果在cache中，那么数据一定在主存中，从而页表一定不会失效。此外，TLB命中，但页表失效也是不可能发生的。因为TLB是对页表的缓存。

##### 虚拟存储的保护

一般而言，为了是操作系统在虚拟存储系统中实现保护，硬件需要提供这三种基本能力：

- 支持至少两种模式，即用户进程和操作系统进程。

- 提供一部分用户可读，但不可写的处理器状态。

- 提供在两种模式之间进行转换的机制。

并且，如果我们不希望，各个进程之间的数据不应该能够互相读取。

因此，在进程切换的时候，我们需要将TLB清空，并且将页表寄存器指向新进程的页表。如果我们想要允许共享，那么可以在新进程的页表后添加需要的表项，并且设定是否具有修改权限等问题。

##### 处理TLB失效和缺页失效

缺页失效的处理流程：

- 用虚拟地址找到对应的页表表项。并且，在辅助存储中找到它。

- 选择要在主存中替换的物理页。如果该页是脏的，那么需要写入。

- 进行读操作。把数据提取到主存中。

然后，指令由内核态重置为用户态，原来的指令流开始重新启动。

#### 存储结构的一般框架

##### 块如何放置？

- 直接映射

- 全相联

- 组相联

##### 如何找到块？

- 直接映射

- 组相联

- 全相联

全相联的一些好处：

- 全相联的是好的。

- 全相联可以允许使用复杂的替换策略。

- 全相联易于索引。

##### 如何替换？

- 随机

- LRU

在相联度不低的情况下，实现LRU的代价是很高的。因为追踪需要很多的时间。这种情况下的LRU，一般是一种近似的LRU。对于2路组的cache而言，随机策略比LRU的失效率高约1.1倍。随着cache容量的增大，两种策略的失效率都下降，绝对差异也变小。

##### 如何写？

- 写穿透

- 写返回

写返回的好处：

- 处理器的可以按照cache的接收速率进行写。

- 块内的多次写操作，只需要对存储结构中的较低层进行一次写操作。

- 写回块时，由于可以写一整个块，所以系统可以有效地利用高带宽传输。

- 失效处理比较简单，代价也较小。

但是写返回比写穿透更难实现。

##### 3C:cache的三种失效

- 强制失效：对在cache中从来没有出现的块进行第一次访问产生的失效。也称为冷启动失效。

- 容量失效：cache无法包含程序执行期间需要的所有快而产生的失效。当某些块被替换出去，再被调入时，会发生容量失效。

- 冲突失效：在如组相连和直接映射cache中，可能会有多个块竞争同一个位置。这种cache失效也被称为碰撞失效。

一些措施以及它们对cache的影响：

- 增加cache容量：降低失效率，但是可能会延长访问时间。

- 增加相联度：减少了冲突失效，从而降低了失效率；可能会延长访问时间。

- 增加块容量：对较宽范围内变化的块大小，失效率降低；但是增加了失效损失，并且如果块过大，也可能增加失效率。

#### Cache一致性

对于多核处理器而言，每一个处理器都可以看到主存上的所有数据。但是，每一个处理器都有自己的cache，如果不对数据作额外的保护，那么不同的处理器可能会看到不同的数据。这个问题一般被称为cache一致性问题。

存储器系统行为需要具有两种性质。

- 一致性(cache coherence)，这规定了读取操作会返回什么值。

- 连续性(memory consistency)，定义了写入的值何时会被读取操作返回。

一致性的要求：

- 一个处理器对一个地址进行了写入，如果在这次写入和下一次读取操作之间有足够的时间间隔，并且没有再次对这个地址进行写入，那么读取到的值一定是该处理器写入的值。

- 对同一位置的写入操作是串行的。

##### 实现一致性的基本方案

支持cache一致性的多处理器中，cache应当做到：

- 迁移。数据可以移动到本地cache，并且能够被以透明的方式使用。也就是说，每一个cache可以独自使用自己的那一部分。

- 复制。读取共享数据是，本地cache会创建数据项的副本。

支持这两点的cache对性能非常重要。我们需要cache一致性协议，来保证多个处理器之间数据的一致性。

##### 监听协议

最朴素的一种方式，我们称之为写无效协议。一个处理器对一个地址进行写入后，会向其他处理器发出一个无效信号，标记这一部分数据失效，从而迫使这些cache去获得最新版本的数据。对于写穿透型，这一步是平凡可做的。对于写返回，我们需要更好地把控将数据从cache中写回的时间。也就是说，对于共享的数据，一旦其他cache发出了对已持有的modified的共享数据的请求，就应该写回。

除了监听协议之外，目前更常用的是目录协议。

## 并行处理器

任务级并行（进程级并行）：多任务，单线程，相互独立。

并行处理程序：同时在多个处理器上运行的程序。

目前也有采用集群方式的结构。

目前的多处理器，一般称之为多核微处理器。它们一般都是共享内存处理器。

要想写出一个好的并行处理程序还是比较难的。根据Amdahl定律：

$$
优化后时间=\frac{受优化影响的执行时间}{优化量}+不受优化影响的时间\\
加速比=\frac{1}{(1-受优化影响的执行时间占比)+\frac{受优化影响的执行时间占比}{优化量}}
$$

如果想在多处理器上获得比较好的加速比，那么问题的规模应当比较大。

我们定义，在保持问题规模不变的同时测量的加速比为强比例缩放，值约为问题规模与处理器数量之比$M/P$。我们定义，在保持问题规模与处理器数量成比例增长时，测量出的加速比为弱比例缩放，其值大约为问题规模$M$

同时，我们也希望多核处理器可以负载均衡。

### 不同的处理器类别

- SISD:单指令流，单数据流(Intel Pentium 4，传统单处理器使用)

- MISD:多指令流，单数据流(暂无实例)

- SIMD:单指令流，多数据流(x86 SSE)

- MIMD:多指令流，多数据流(Intel Core i7，传统多处理器使用)

#### SPMD

SPMD意为单程序多数据流。它是在MIMD计算机上编程的一种常用方法。不同的条件语句会在不同的处理器上执行。

#### SIMD

单个的SIMD指令，可以同时操控多个数据向量。其中每一条并行执行单元都是同步的。

SIMD在处理for循环时效果很好。为了能够在SIMD上高效并行运行程序，程序中必须有着合适的数据。这种并行称之为数据级并行。

#### 向量机

对SIMD更早，更优雅的解释被称为向量体系结构。

比如说，在向量化的riscv下，一次向量操作可以处理64个数据元。一个向量寄存器中可以存放多个数据元进行操作。

向量化处理存在着一些明显的好处。

- 一个向量指令指定了大量的工作。

- 向量指令可以保证运算结果之间的互相独立性。

- 相比于MIMD，向量体系结构和编译器更容易写出高效的程序。

- 硬件不必检查向量指令内部的数据冲突，减少耗时。

- 访问存储器的向量指令访问模式固定。主存延时开销小。

- 循环通常可以被一些向量指令取代，循环导致的控制冒险可能消失。

- 向量指令可以减少代码长度、冒险检查，并且有效利用了存储器带宽。

向量算术指令通常只允许两个处于相同下标处的值进行运算。我们称一部分向量功能单元和一部分向量寄存器堆构成了一个向量通道。什么意思呢？比如说，我们的向量位数为64位，那么我们可以分成$4k,4k+1,4k+2,4k+3$四条向量通道，分别对对应的部分进行操作。

### 硬件多线程

硬件多线程是和MIMD相关的一个概念。MIMD依赖于多个进程或者进程，从而使多个处理器持续工作。硬件多线程则允许**多个**线程通过重叠的方式共享单个处理器的功能单元。硬件多线程一般有这几种实现方法。

- 细粒度多线程。每条指令执行后立即进行线程交换。如果一个线程指令在当前时刻停顿，那么按顺序执行下一个线程的对应指令。细粒度多线程的好处在于可以隐藏较多的停顿引起的吞吐量损失。坏处在于，它会降低单个线程的执行速度。

- 粗粒度多线程。粗粒度多线程，指仅在高开销的停顿下进行线程的切换，比如末级cache失效时等情况。它的缺点在于降低吞吐量损失的能力有限，尤其是对于那些较短的停顿。这是因为多线程的**流水线启动开销**。进行线程切换时，流水线必须被清空或者冻结。

- 同时多线程(SMT)。它使用多发射，动态调度流水线的处理器资源来挖掘线程级和指令级的并行。它可以通过重命名等方式，尝试去降低流水线的启动开销等。SMT始终都在执行来自于多个线程的指令。

在SMT中，线程级并行和指令级的并行都得到了充分的利用。

### 共享内存多处理器(SMP)

其为所有的处理器提供了统一的物理地址空间。

SMP一般具有两种类型。

- 统一内存访问(UMA)多处理器。无论哪个处理器提出访存请求，耗时都基本相近。

- 非统一内存访问(NUMA)多处理器。对于不同的处理器而言，有一些地址对于某些处理器而言，访问起来会快不少。这是因为主存被划分，并且分配给一不同的处理器或者内存控制器。

处理器并行执行时，通常需要共享数据。因此，数据之间必须进行协调，即**同步**。一种简单的方式是通过锁来解决这个问题。同一时刻只能有一个处理器获得锁，其他想要操作共享数据的处理器必须等待，直到该处理器解锁共享变量为止。

### GPU

图形处理单元(Graphics Processing Unit)

GPU作为的是CPU补充的加速器。因此，它不需要能够执行CPU所能执行的所有任务。同时，GPU的问题规模一般是MB-GB数量级的，相对比较小。

GPU和CPU较大的差异：

- GPU不依赖于多级cache来消除内存的长延迟，CPU依赖。GPU通过硬件多线程来隐藏内存延迟。在存储器发出请求和数据到达的时间内，GPU执行了成百上千个独立的进程。

- GPU存储器注重带宽而不是延迟。它的DRAM甚至是被特化过的。也就是说，具有更高的带宽，但是容量却可能更小一些。

- GPU需要多线程来获得良好的内存带宽。因此，除去多线程，GPU还可以容纳许多并行处理器(MIMD)。每个GPU处理器都比传统CPU有更多的线程和处理器。

CUDA(Compute Unified Device Architecture)可以让程序员编写在GPU上运行的C程序。NVIDIA把所有形式的并行都定义为了CUDA线程，并且用这种最底层的并行作为编程的原语。

和向量体系结构一样，GPU**仅**适用于数据级并行问题。但是，GPU可以有更多的寄存器，并且GPU具有硬件多线程功能。

GPU可以看成是多线程SIMD处理器组成的MIMD.

硬件创建、管理、调度、执行的机器对象是一个SIMD指令的线程。它是一个只包含SIMD指令的线程。每一个SIMD线程有自己的PC，从而运行在多线程的SIMD处理器上。SIMD线程调度器包含一个控制器，这个控制器清楚SIMD的那些线程已经准备好运行，并且讲那些已经准备好运行的线程发送到调度单元，从而在多线程SIMD处理器上运行。SIMD线程调度器和传统的多线程处理器中的硬件线程调度器相同，区别在于它只调度SIMD指令的线程。

在GPU硬件中，总共有两级硬件调度器。

1. 线程块调度器。用于多线程处理器分配线程块。

2. SIMD线程调度器，其位于SIMD处理器内部，可以在SIMD线程运行时进行调度。

GPU的特点是SIMD处理器数量多，cache容量和主存容量都比较小，不支持动态页面调度，也不对cache一致性作出任何保证。

Dennard Scaling:随着晶体管尺寸缩小，其功率密度基本保持不变，从而芯片功率与芯片面积成正比。（目前似乎已经失效？）

领域定制体系结构(DSA)目前被认为是改善性能和能效的唯一途径。DSA只能完成特定范围的任务，但是效果很好。DSA设计的几个原则是：

- 使用专用存储器来最小化数据移动距离。

- 放弃对微结构的深度优化，节省资源投入到更多的ALU或者更大的内存上。

- 采用满足要求的最简单并行方式。

- 减少数据大小和数据类型。

- 使用领域定制的编程语言。

Google的TPU(Tensor Processing Unit)就是一种面向DNN的DSA。它的处理性能更高，同时功耗也更新。

#### 具有私有地址的多处理器

采用这种方案的多处理器，必须通过显式的消息传递进行通信。系统必须具有发送消息例程和接收消息例程来进行通信。

##### 集群

目前消息传递并行计算机中最普遍的实例。它指的是通过标准网络交换机上的I/O进行链接的计算机集合，从而构成的消息传递多处理器。集群的好处在于，具有较高的可靠性。一台计算机的故障不会影响整个系统。此外，集群的成本也相对较低。但是，它的通信性能比较差。

##### 仓储级计算机(WSC)

可以被看做是一个架构和操作更加复杂的大型计算机集群。

WSC的使用场景，是大量、简单的并行性下的工作，比如请求级并行等。同时，WSC架构师也需要考虑运营的成本，因为WSC运营成本远超服务器；WSC架构师也需要考虑多少才是合适的规模。

#### 多处理器网络拓扑

因为不同的核心之间需要进行通信。

我们通过总网络带宽和二分带宽两个值来对多处理器网络进行评估。

总网络带宽，指的是每个链路的带宽，乘上链路的总数量。

二分带宽，只能说把机器分成两组来计算网络带宽。然后，把分界线上的带宽相加。在选取分界线时，我们要选择会使得二分带宽值更小的那种划分，从而更好地评估网络的最坏性能。

##### 环

也就是一个循环链表。它的特点是允许多个点进行同时传输。

##### 全连接网络

成本巨大。但你就说快不快吧。

##### 商用并行处理器中的结构

网格、节点树等拓扑方式。

多级网络：在每个节点上提供小型开关的网络。

交叉开关网络(全连接)：允许任何节点通过网络时和任何其他节点通信。
